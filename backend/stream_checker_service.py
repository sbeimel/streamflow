#!/usr/bin/env python3
"""
Stream Checker Service for Dispatcharr.

This service manages stream quality checking, rating, and ordering for
Dispatcharr channels. It implements a comprehensive system for maintaining
optimal stream quality across all channels.

Features:
    - Queue-based channel checking with priority support
    - Tracking of M3U playlist update events
    - Scheduled global checks during configurable off-peak hours
    - Progressive stream rating and automatic ordering
    - Real-time progress reporting via web API
    - Thread-safe operations with proper synchronization

The service runs continuously in the background, monitoring for channel
updates and maintaining a queue of channels that need checking. It
integrates with the stream_check_utils.py module for stream analysis.
"""

import json
import logging
import os
import threading
import time
from collections import defaultdict, deque, Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
import queue

from api_utils import (
    fetch_channel_streams,
    update_channel_streams,
    _get_base_url,
    patch_request
)

# Import UDI for direct data access
from udi import get_udi_manager

# Import dead streams tracker
from dead_streams_tracker import DeadStreamsTracker

# Import channel settings manager
from channel_settings_manager import get_channel_settings_manager

# Import profile config
from profile_config import get_profile_config

# Import centralized stream stats utilities
from stream_stats_utils import (
    parse_bitrate_value,
    format_bitrate,
    parse_fps_value,
    format_fps,
    extract_stream_stats,
    format_stream_stats_for_display,
    calculate_channel_averages,
    is_stream_dead as utils_is_stream_dead
)

# Import changelog manager
try:
    from automated_stream_manager import ChangelogManager
    CHANGELOG_AVAILABLE = True
except ImportError:
    CHANGELOG_AVAILABLE = False

# Setup centralized logging
from logging_config import setup_logging, log_function_call, log_function_return, log_exception, log_state_change

logger = setup_logging(__name__)

# Configuration directory
CONFIG_DIR = Path(os.environ.get('CONFIG_DIR', '/app/data'))


class StreamCheckConfig:
    """Configuration for stream checking service."""
    
    DEFAULT_CONFIG = {
        'enabled': True,
        'check_interval': 300,  # DEPRECATED - checks now only triggered by M3U refresh
        # Individual automation controls
        'automation_controls': {
            'auto_m3u_updates': True,  # Automatically refresh M3U playlists
            'auto_stream_matching': True,  # Automatically match streams to channels via regex
            'auto_quality_checking': True,  # Automatically check stream quality
            'scheduled_global_action': False,  # Run scheduled global actions (update + match + check all)
            'remove_non_matching_streams': False  # Remove streams from channels if they no longer match regex
        },
        'global_check_schedule': {
            'enabled': True,
            'cron_expression': '0 3 * * *',  # Cron expression: default is daily at 3:00 AM
            'frequency': 'daily',  # DEPRECATED: kept for backward compatibility - 'daily' or 'monthly'
            'hour': 3,  # DEPRECATED: kept for backward compatibility - 3 AM for off-peak checking
            'minute': 0,  # DEPRECATED: kept for backward compatibility
            'day_of_month': 1  # DEPRECATED: kept for backward compatibility - Day of month for monthly checks (1-31)
        },
        'stream_analysis': {
            'ffmpeg_duration': 30,  # seconds to analyze each stream
            'timeout': 30,  # timeout for operations
            'stream_startup_buffer': 10,  # seconds buffer for stream startup (max time before stream starts)
            'retries': 1,  # retry attempts
            'retry_delay': 10,  # seconds between retries
            'user_agent': 'VLC/3.0.14'  # user agent for ffmpeg/ffprobe
        },
        'scoring': {
            'weights': {
                'bitrate': 0.40,
                'resolution': 0.35,
                'fps': 0.15,
                'codec': 0.10
            },
            'min_score': 0.0,  # minimum score to keep stream
            'prefer_h265': True  # prefer h265 over h264
        },
        'queue': {
            'max_size': 1000,
            'check_on_update': True,  # check channels when they receive M3U updates
            'max_channels_per_run': 50  # limit channels per check cycle
        },
        'concurrent_streams': {
            'global_limit': 10,  # Maximum concurrent stream checks globally (0 = unlimited)
            'enabled': True,  # Enable concurrent checking via Celery
            'stagger_delay': 1.0  # Delay in seconds between dispatching tasks to prevent simultaneous starts
        },
        'dead_stream_handling': {
            'enabled': True,  # Enable dead stream removal
            'min_resolution_width': 0,  # Minimum width in pixels (0 = no minimum, e.g., 1280 for 720p)
            'min_resolution_height': 0,  # Minimum height in pixels (0 = no minimum, e.g., 720 for 720p)
            'min_bitrate_kbps': 0,  # Minimum bitrate in kbps (0 = no minimum)
            'min_score': 0  # Minimum score (0-100, 0 = no minimum)
        },
        'account_stream_limits': {
            'enabled': True,  # Enable per-account stream limits for channel assignment
            'global_limit': 0,  # Global limit per account (0 = unlimited)
            'account_limits': {}  # Per-account limits: {account_id: limit}
        },
        'stream_ordering': {
            'provider_diversification': False,  # Enable provider diversification for better redundancy
            'diversification_mode': 'round_robin'  # Mode: 'round_robin' or 'weighted'
        },
        'profile_failover': {
            'enabled': True,  # Enable profile failover for quality checks
            'try_full_profiles': True,  # Try full profiles in Phase 2 with intelligent polling
            'phase2_max_wait': 600,  # Maximum wait time in Phase 2 (seconds, default: 10 minutes)
            'phase2_poll_interval': 10  # Check for free profiles every X seconds (default: 10s)
        }
    }
    
    def __init__(self, config_file: Optional[str] = None) -> None:
        """
        Initialize the StreamCheckConfig.
        
        Parameters:
            config_file (Optional[str]): Path to config file. Defaults
                to CONFIG_DIR/stream_checker_config.json.
        """
        if config_file is None:
            config_file = CONFIG_DIR / 'stream_checker_config.json'
        self.config_file = Path(config_file)
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """
        Load configuration from file or create default.
        
        Merges loaded config with DEFAULT_CONFIG to ensure all
        required keys exist even if config file is incomplete.
        
        Returns:
            Dict[str, Any]: The configuration dictionary.
        """
        import copy
        log_function_call(logger, "_load_config", config_file=str(self.config_file))
        
        if self.config_file.exists():
            logger.debug(f"Config file exists: {self.config_file}")
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    loaded = json.load(f)
                    logger.debug(f"Loaded config with {len(loaded)} top-level keys")
                    # Deep copy defaults to avoid mutating DEFAULT_CONFIG
                    config = copy.deepcopy(self.DEFAULT_CONFIG)
                    config.update(loaded)
                    
                    # Auto-migrate legacy pipeline mode to automation_controls
                    pipeline_mode = config.get('pipeline_mode', '')
                    if pipeline_mode and pipeline_mode != 'disabled':
                        logger.info(f"Migrating legacy pipeline mode '{pipeline_mode}' to automation_controls")
                        
                        # Map pipeline modes to automation controls
                        if pipeline_mode == 'pipeline_1':
                            config['automation_controls'] = {
                                'auto_m3u_updates': True,
                                'auto_stream_matching': True,
                                'auto_quality_checking': True,
                                'scheduled_global_action': False
                            }
                        elif pipeline_mode == 'pipeline_1_5':
                            config['automation_controls'] = {
                                'auto_m3u_updates': True,
                                'auto_stream_matching': True,
                                'auto_quality_checking': True,
                                'scheduled_global_action': True
                            }
                        elif pipeline_mode == 'pipeline_2':
                            config['automation_controls'] = {
                                'auto_m3u_updates': True,
                                'auto_stream_matching': True,
                                'auto_quality_checking': False,
                                'scheduled_global_action': False
                            }
                        elif pipeline_mode == 'pipeline_2_5':
                            config['automation_controls'] = {
                                'auto_m3u_updates': True,
                                'auto_stream_matching': True,
                                'auto_quality_checking': False,
                                'scheduled_global_action': True
                            }
                        elif pipeline_mode == 'pipeline_3':
                            config['automation_controls'] = {
                                'auto_m3u_updates': False,
                                'auto_stream_matching': False,
                                'auto_quality_checking': False,
                                'scheduled_global_action': True
                            }
                        
                        # Remove pipeline_mode key
                        config.pop('pipeline_mode', None)
                        
                        # Save migrated config
                        self._save_config(config)
                        logger.info(f"Successfully migrated to automation_controls: {config['automation_controls']}")
                    
                    logger.debug(f"Merged config: automation_controls={config.get('automation_controls')}, enabled={config.get('enabled')}")
                    log_function_return(logger, "_load_config", f"<config with {len(config)} keys>")
                    return config
            except (json.JSONDecodeError, FileNotFoundError) as e:
                log_exception(logger, e, f"loading config from {self.config_file}")
                logger.warning(
                    f"Could not load config from "
                    f"{self.config_file}: {e}, using defaults"
                )
        else:
            logger.debug(f"Config file does not exist: {self.config_file}")
        
        # Create default config - use deep copy to avoid mutation
        logger.debug("Creating default config")
        self._save_config(copy.deepcopy(self.DEFAULT_CONFIG))
        log_function_return(logger, "_load_config", "<default config>")
        return copy.deepcopy(self.DEFAULT_CONFIG)
    
    def _save_config(
        self, config: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Save configuration to file.
        
        Parameters:
            config (Optional[Dict[str, Any]]): Config to save.
                Defaults to self.config.
        """
        if config is None:
            config = self.config
        
        self.config_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)
    
    def update(self, updates: Dict[str, Any]) -> None:
        """
        Update configuration with new values.
        
        Performs deep update to handle nested dictionaries.
        
        Parameters:
            updates (Dict[str, Any]): Configuration updates to apply.
        """
        def deep_update(
            base: Dict[str, Any], updates: Dict[str, Any]
        ) -> None:
            """Recursively update nested dictionaries."""
            for key, value in updates.items():
                if (isinstance(value, dict) and key in base and
                        isinstance(base[key], dict)):
                    deep_update(base[key], value)
                else:
                    base[key] = value
        
        deep_update(self.config, updates)
        self._save_config()
        logger.info("Stream checker configuration updated")
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        Get configuration value using dot notation.
        
        Supports nested keys like 'queue.max_size'.
        
        Parameters:
            key (str): Configuration key (supports dot notation).
            default (Any): Default value if key not found.
            
        Returns:
            Any: The configuration value or default.
        """
        keys = key.split('.')
        value = self.config
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
            else:
                return default
        return value if value is not None else default
    
    def is_auto_m3u_updates_enabled(self) -> bool:
        """Check if automatic M3U updates are enabled."""
        return self.config.get('automation_controls', {}).get('auto_m3u_updates', True)
    
    def is_auto_stream_matching_enabled(self) -> bool:
        """Check if automatic stream matching is enabled."""
        return self.config.get('automation_controls', {}).get('auto_stream_matching', True)
    
    def is_auto_quality_checking_enabled(self) -> bool:
        """Check if automatic quality checking is enabled."""
        return self.config.get('automation_controls', {}).get('auto_quality_checking', True)
    
    def is_scheduled_global_action_enabled(self) -> bool:
        """Check if scheduled global action is enabled."""
        return self.config.get('automation_controls', {}).get('scheduled_global_action', False)


class ChannelUpdateTracker:
    """Tracks which channels have received M3U updates."""
    
    def __init__(self, tracker_file=None):
        if tracker_file is None:
            tracker_file = CONFIG_DIR / 'channel_updates.json'
        self.tracker_file = Path(tracker_file)
        self.updates = self._load_updates()
        self.lock = threading.Lock()
        # Ensure the file is created on initialization
        self._save_updates()
    
    def _load_updates(self) -> Dict:
        """Load update tracking data."""
        if self.tracker_file.exists():
            try:
                with open(self.tracker_file, 'r') as f:
                    return json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                logger.warning(f"Could not load updates from {self.tracker_file}, creating new")
        return {'channels': {}, 'last_global_check': None}
    
    def _save_updates(self):
        """Save update tracking data."""
        try:
            self.tracker_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.tracker_file, 'w') as f:
                json.dump(self.updates, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save channel updates: {e}")
    
    def mark_channel_updated(self, channel_id: int, timestamp: str = None, stream_count: int = None):
        """Mark a channel as having received an update.
        
        Args:
            channel_id: The channel ID to mark as updated
            timestamp: When the update occurred (defaults to now)
            stream_count: Number of streams in the channel after update
        """
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        with self.lock:
            if 'channels' not in self.updates:
                self.updates['channels'] = {}
            
            channel_key = str(channel_id)
            
            # Always mark channel as needing check if stream count changed
            # This ensures new streams are analyzed even during invulnerability period
            if channel_key in self.updates['channels']:
                channel_info = self.updates['channels'][channel_key]
                # Preserve checked_stream_ids if they exist
                checked_stream_ids = channel_info.get('checked_stream_ids', [])
                
                self.updates['channels'][channel_key] = {
                    'last_update': timestamp,
                    'needs_check': True,
                    'stream_count': stream_count,
                    'checked_stream_ids': checked_stream_ids
                }
            else:
                self.updates['channels'][channel_key] = {
                    'last_update': timestamp,
                    'needs_check': True,
                    'stream_count': stream_count,
                    'checked_stream_ids': []
                }
            self._save_updates()
    
    def mark_channels_updated(self, channel_ids: List[int], timestamp: str = None, stream_counts: Dict[int, int] = None):
        """Mark multiple channels as updated.
        
        Args:
            channel_ids: List of channel IDs to mark
            timestamp: When the update occurred (defaults to now)
            stream_counts: Optional dict mapping channel_id to stream count
        """
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        if stream_counts is None:
            stream_counts = {}
        
        marked_count = 0
        
        with self.lock:
            if 'channels' not in self.updates:
                self.updates['channels'] = {}
            
            for channel_id in channel_ids:
                channel_key = str(channel_id)
                stream_count = stream_counts.get(channel_id)
                
                # Always mark channel if stream count changed (new streams added)
                # Preserve checked_stream_ids if they exist
                if channel_key in self.updates['channels']:
                    channel_info = self.updates['channels'][channel_key]
                    checked_stream_ids = channel_info.get('checked_stream_ids', [])
                    
                    self.updates['channels'][channel_key] = {
                        'last_update': timestamp,
                        'needs_check': True,
                        'stream_count': stream_count,
                        'checked_stream_ids': checked_stream_ids
                    }
                else:
                    self.updates['channels'][channel_key] = {
                        'last_update': timestamp,
                        'needs_check': True,
                        'stream_count': stream_count,
                        'checked_stream_ids': []
                    }
                marked_count += 1
            
            if marked_count > 0:
                self._save_updates()
        
        logger.info(f"Marked {marked_count} channels as updated")
    
    def get_channels_needing_check(self) -> List[int]:
        """Get list of channel IDs that need checking (read-only, doesn't clear flag).
        
        For actual queueing operations, use get_and_clear_channels_needing_check() instead
        to prevent race conditions.
        """
        with self.lock:
            channels = []
            for channel_id, info in self.updates.get('channels', {}).items():
                if info.get('needs_check', False):
                    channels.append(int(channel_id))
            return channels
    
    def get_and_clear_channels_needing_check(self, max_channels: int = None) -> List[int]:
        """Get list of channel IDs that need checking and atomically clear their needs_check flag.
        
        This atomic operation prevents race conditions where M3U refresh could
        re-mark channels while they're being queued.
        
        Args:
            max_channels: Maximum number of channels to return (None = all)
            
        Returns:
            List of channel IDs that were marked as needing check
        """
        with self.lock:
            channels = []
            timestamp = datetime.now().isoformat()
            
            for channel_id, info in self.updates.get('channels', {}).items():
                if info.get('needs_check', False):
                    channels.append(int(channel_id))
                    # Clear the flag immediately
                    info['needs_check'] = False
                    info['queued_at'] = timestamp
                    
                    if max_channels and len(channels) >= max_channels:
                        break
            
            # Filter channels by checking_mode setting (channel-level overrides group-level)
            # Need to get full channel data to access channel_group_id
            channel_settings = get_channel_settings_manager()
            udi = get_udi_manager()
            
            filtered_channels = []
            for cid in channels:
                # Get channel data to access group_id
                channel_data = None
                for ch in udi.get_channels():
                    if ch.get('id') == cid:
                        channel_data = ch
                        break
                
                if channel_data:
                    channel_group_id = channel_data.get('channel_group_id')
                    
                    # Check if channel has an explicit setting (not default)
                    channel_explicit_settings = channel_settings._settings.get(cid, {})
                    has_explicit_checking = 'checking_mode' in channel_explicit_settings
                    
                    if has_explicit_checking:
                        # Channel has explicit override - use it
                        if channel_settings.is_checking_enabled(cid):
                            filtered_channels.append(cid)
                    else:
                        # No channel override - use group setting (or default to enabled if no group)
                        if channel_settings.is_channel_enabled_by_group(channel_group_id, mode='checking'):
                            filtered_channels.append(cid)
                else:
                    # If we can't find channel data, use channel-level setting only
                    if channel_settings.is_checking_enabled(cid):
                        filtered_channels.append(cid)
            
            excluded_count = len(channels) - len(filtered_channels)
            
            if excluded_count > 0:
                logger.info(f"Excluding {excluded_count} channel(s) with checking disabled (channel or group level)")
            
            if filtered_channels:
                self._save_updates()
                logger.debug(f"Atomically retrieved and cleared {len(filtered_channels)} channels needing check")
            
            return filtered_channels
    
    def mark_channel_checked(self, channel_id: int, timestamp: str = None, stream_count: int = None, checked_stream_ids: List[int] = None):
        """Mark a channel as checked (completed).
        
        Args:
            channel_id: The channel ID to mark as checked
            timestamp: When the check was completed (defaults to now)
            stream_count: Number of streams in the channel
            checked_stream_ids: List of stream IDs that were checked
        """
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        with self.lock:
            if 'channels' not in self.updates:
                self.updates['channels'] = {}
            
            channel_key = str(channel_id)
            if channel_key in self.updates['channels']:
                # Update existing entry
                self.updates['channels'][channel_key]['needs_check'] = False
                self.updates['channels'][channel_key]['last_check'] = timestamp
                if stream_count is not None:
                    self.updates['channels'][channel_key]['stream_count'] = stream_count
                if checked_stream_ids is not None:
                    self.updates['channels'][channel_key]['checked_stream_ids'] = checked_stream_ids
            else:
                # Create new entry
                self.updates['channels'][channel_key] = {
                    'needs_check': False,
                    'last_check': timestamp,
                    'stream_count': stream_count,
                    'checked_stream_ids': checked_stream_ids if checked_stream_ids is not None else []
                }
            self._save_updates()
    
    def get_checked_stream_ids(self, channel_id: int) -> List[int]:
        """Get the list of stream IDs that have been checked for a channel.
        
        Args:
            channel_id: The channel ID to query
            
        Returns:
            List of stream IDs that have been checked (empty list if none or channel not tracked)
        """
        with self.lock:
            channel_key = str(channel_id)
            if channel_key in self.updates.get('channels', {}):
                return self.updates['channels'][channel_key].get('checked_stream_ids', [])
            return []
    
    def mark_channel_for_force_check(self, channel_id: int):
        """Mark a channel for force checking (bypasses 2-hour immunity).
        
        Args:
            channel_id: The channel ID to mark for force check
        """
        with self.lock:
            if 'channels' not in self.updates:
                self.updates['channels'] = {}
            
            channel_key = str(channel_id)
            if channel_key not in self.updates['channels']:
                self.updates['channels'][channel_key] = {}
            
            self.updates['channels'][channel_key]['force_check'] = True
            self._save_updates()
    
    def should_force_check(self, channel_id: int) -> bool:
        """Check if a channel should be force checked (bypassing immunity).
        
        Args:
            channel_id: The channel ID to check
            
        Returns:
            True if force check is enabled for this channel
        """
        with self.lock:
            channel_key = str(channel_id)
            if channel_key in self.updates.get('channels', {}):
                return self.updates['channels'][channel_key].get('force_check', False)
            return False
    
    def clear_force_check(self, channel_id: int):
        """Clear the force check flag for a channel.
        
        Args:
            channel_id: The channel ID to clear force check for
        """
        with self.lock:
            channel_key = str(channel_id)
            if channel_key in self.updates.get('channels', {}):
                self.updates['channels'][channel_key]['force_check'] = False
                self._save_updates()
    
    def mark_global_check(self, timestamp: str = None):
        """Mark that a global check was initiated.
        
        This only updates the timestamp to prevent duplicate global checks.
        It does NOT clear needs_check flags - those should only be cleared
        when channels are actually checked via mark_channel_checked().
        """
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        with self.lock:
            self.updates['last_global_check'] = timestamp
            self._save_updates()
    
    def get_last_global_check(self) -> Optional[str]:
        """Get timestamp of last global check."""
        return self.updates.get('last_global_check')


class StreamCheckQueue:
    """Queue manager for channel stream checking."""
    
    def __init__(self, max_size=1000):
        self.queue = queue.Queue(maxsize=max_size)
        self.queued = set()  # Track channels already in queue
        self.in_progress = set()
        self.completed = set()
        self.failed = {}
        self.lock = threading.Lock()
        self.stats = {
            'total_queued': 0,
            'total_completed': 0,
            'total_failed': 0,
            'current_channel': None,
            'queue_size': 0
        }
    
    def add_channel(self, channel_id: int, priority: int = 0):
        """Add a channel to the checking queue."""
        with self.lock:
            # Check if channel is already queued, in progress, or completed
            if channel_id not in self.queued and channel_id not in self.in_progress and channel_id not in self.completed:
                try:
                    self.queue.put((priority, channel_id), block=False)
                    self.queued.add(channel_id)
                    self.stats['total_queued'] += 1
                    self.stats['queue_size'] = self.queue.qsize()
                    logger.debug(f"Added channel {channel_id} to queue (priority: {priority})")
                    return True
                except queue.Full:
                    logger.warning(f"Queue is full, cannot add channel {channel_id}")
                    return False
        return False
    
    def add_channels(self, channel_ids: List[int], priority: int = 0):
        """Add multiple channels to the queue."""
        added = 0
        for channel_id in channel_ids:
            if self.add_channel(channel_id, priority):
                added += 1
        logger.info(f"Added {added}/{len(channel_ids)} channels to checking queue")
        return added
    
    def remove_from_completed(self, channel_id: int):
        """Remove a channel from the completed set to allow re-queueing.
        
        This is used when a channel receives new streams and needs to be
        checked again, even if it was previously completed.
        """
        with self.lock:
            if channel_id in self.completed:
                self.completed.discard(channel_id)
                logger.debug(f"Removed channel {channel_id} from completed set")
                return True
        return False
    
    def get_next_channel(self, timeout: float = 1.0) -> Optional[int]:
        """Get the next channel to check."""
        try:
            priority, channel_id = self.queue.get(timeout=timeout)
            with self.lock:
                self.queued.discard(channel_id)  # Remove from queued set
                self.in_progress.add(channel_id)
                self.stats['current_channel'] = channel_id
                self.stats['queue_size'] = self.queue.qsize()
            return channel_id
        except queue.Empty:
            return None
    
    def mark_completed(self, channel_id: int):
        """Mark a channel check as completed."""
        with self.lock:
            if channel_id in self.in_progress:
                self.in_progress.remove(channel_id)
            self.completed.add(channel_id)
            self.stats['total_completed'] += 1
            if self.stats['current_channel'] == channel_id:
                self.stats['current_channel'] = None
            logger.debug(f"Marked channel {channel_id} as completed")
    
    def mark_failed(self, channel_id: int, error: str):
        """Mark a channel check as failed."""
        with self.lock:
            if channel_id in self.in_progress:
                self.in_progress.remove(channel_id)
            self.failed[channel_id] = {
                'error': error,
                'timestamp': datetime.now().isoformat()
            }
            self.stats['total_failed'] += 1
            if self.stats['current_channel'] == channel_id:
                self.stats['current_channel'] = None
            logger.warning(f"Marked channel {channel_id} as failed: {error}")
    
    def get_status(self) -> Dict:
        """Get current queue status."""
        with self.lock:
            return {
                'queue_size': self.queue.qsize(),
                'queued': len(self.queued),
                'in_progress': len(self.in_progress),
                'completed': len(self.completed),
                'failed': len(self.failed),
                'current_channel': self.stats['current_channel'],
                'total_queued': self.stats['total_queued'],
                'total_completed': self.stats['total_completed'],
                'total_failed': self.stats['total_failed']
            }
    
    def clear(self):
        """Clear the queue and reset stats."""
        with self.lock:
            while not self.queue.empty():
                try:
                    self.queue.get_nowait()
                except queue.Empty:
                    break
            self.queued.clear()
            self.in_progress.clear()
            self.completed.clear()
            self.failed.clear()
            self.stats = {
                'total_queued': 0,
                'total_completed': 0,
                'total_failed': 0,
                'current_channel': None,
                'queue_size': 0
            }
        logger.info("Queue cleared")


class StreamCheckerProgress:
    """Manages progress tracking for stream checker operations."""
    
    def __init__(self, progress_file=None):
        if progress_file is None:
            progress_file = CONFIG_DIR / 'stream_checker_progress.json'
        self.progress_file = Path(progress_file)
        self.lock = threading.Lock()
    
    def update(self, channel_id: int, channel_name: str, current: int, total: int,
               current_stream: str = '', status: str = 'checking', step: str = '', step_detail: str = ''):
        """Update progress information.
        
        Args:
            channel_id: The ID of the channel being checked
            channel_name: The name of the channel
            current: Current stream number being processed
            total: Total number of streams
            current_stream: Name of the current stream
            status: Overall status (checking, analyzing, updating, etc.)
            step: Current step in the process (e.g., "Fetching streams", "Analyzing", "Scoring", "Reordering")
            step_detail: Additional detail about the current step
        """
        with self.lock:
            progress_data = {
                'channel_id': channel_id,
                'channel_name': channel_name,
                'current_stream': current,
                'total_streams': total,
                'percentage': round((current / total * 100) if total > 0 else 0, 1),
                'current_stream_name': current_stream,
                'status': status,
                'step': step,
                'step_detail': step_detail,
                'timestamp': datetime.now().isoformat()
            }
            
            self.progress_file.parent.mkdir(parents=True, exist_ok=True)
            try:
                with open(self.progress_file, 'w') as f:
                    json.dump(progress_data, f)
                    f.flush()
                    os.fsync(f.fileno())
            except Exception as e:
                logger.warning(f"Failed to write progress file: {e}")
    
    def clear(self):
        """Clear progress tracking."""
        with self.lock:
            if self.progress_file.exists():
                try:
                    self.progress_file.unlink()
                except Exception as e:
                    logger.warning(f"Failed to delete progress file: {e}")
    
    def get(self) -> Optional[Dict]:
        """Get current progress."""
        with self.lock:
            if self.progress_file.exists():
                try:
                    with open(self.progress_file, 'r') as f:
                        return json.load(f)
                except (json.JSONDecodeError, FileNotFoundError):
                    pass
        return None


class StreamCheckerService:
    """Main service for managing stream checking operations."""
    
    def __init__(self):
        log_function_call(logger, "__init__")
        logger.debug("Initializing StreamCheckerService components...")
        
        self.config = StreamCheckConfig()
        logger.debug(f"Config loaded: pipeline_mode={self.config.get('pipeline_mode')}")
        
        self.update_tracker = ChannelUpdateTracker()
        logger.debug("Update tracker initialized")
        
        self.check_queue = StreamCheckQueue(
            max_size=self.config.get('queue.max_size', 1000)
        )
        logger.debug(f"Check queue initialized with max_size={self.config.get('queue.max_size', 1000)}")
        
        self.progress = StreamCheckerProgress()
        logger.debug("Progress tracker initialized")
        
        self.dead_streams_tracker = DeadStreamsTracker()
        logger.debug("Dead streams tracker initialized")
        
        # Initialize changelog manager
        self.changelog = None
        if CHANGELOG_AVAILABLE:
            try:
                self.changelog = ChangelogManager(changelog_file=CONFIG_DIR / "stream_checker_changelog.json")
                logger.info("Stream checker changelog manager initialized")
            except Exception as e:
                log_exception(logger, e, "changelog initialization")
                logger.warning(f"Failed to initialize changelog manager: {e}")
        
        # Batch changelog tracking
        self.batch_changelog_entries = []
        self.batch_start_time = None
        self.batch_lock = threading.Lock()
        
        self.running = False
        self.checking = False
        self.global_action_in_progress = False
        self.worker_thread = None
        self.scheduler_thread = None
        self.lock = threading.Lock()
        
        # Event for immediate triggering of updated channels check
        self.check_trigger = threading.Event()
        logger.debug("Check trigger event created")
        
        # Event for immediate config change notification
        self.config_changed = threading.Event()
        logger.debug("Config changed event created")
        
        logger.info("Stream Checker Service initialized")
        log_function_return(logger, "__init__")
    
    def start(self):
        """Start the stream checker service."""
        log_function_call(logger, "start")
        with self.lock:
            if self.running:
                logger.warning("Stream checker service is already running")
                return
            
            log_state_change(logger, "stream_checker_service", "stopped", "starting")
            self.running = True
            
            # Start worker thread for processing queue
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            logger.debug(f"Worker thread started (id: {self.worker_thread.ident})")
            
            # Start scheduler thread for periodic checks
            self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
            self.scheduler_thread.start()
            logger.debug(f"Scheduler thread started (id: {self.scheduler_thread.ident})")
            
            log_state_change(logger, "stream_checker_service", "starting", "running")
            logger.info("Stream checker service started")
            log_function_return(logger, "start")
    
    def stop(self):
        """Stop the stream checker service."""
        with self.lock:
            if not self.running:
                logger.warning("Stream checker service is not running")
                return
            
            self.running = False
            logger.info("Stream checker service stopping...")
        
        # Wait for threads to finish
        if self.worker_thread and self.worker_thread.is_alive():
            self.worker_thread.join(timeout=5)
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            self.scheduler_thread.join(timeout=5)
        
        self.progress.clear()
        logger.info("Stream checker service stopped")
    
    def _worker_loop(self):
        """Main worker loop for processing the check queue."""
        log_function_call(logger, "_worker_loop")
        logger.info("Stream checker worker started")
        
        while self.running:
            try:
                logger.debug("Worker waiting for next channel from queue...")
                channel_id = self.check_queue.get_next_channel(timeout=1.0)
                if channel_id is None:
                    # No channel in queue - check if we should finalize a batch
                    if self.batch_start_time is not None:
                        # Queue is empty and we have an active batch - finalize it
                        self._finalize_batch_changelog()
                    logger.debug("No channel in queue (timeout)")
                    continue
                
                # Start a new batch if not already started
                if self.batch_start_time is None:
                    self._start_batch_changelog()
                
                logger.debug(f"Worker processing channel {channel_id}")
                # Check this channel
                self._check_channel(channel_id)
                logger.debug(f"Worker completed channel {channel_id}")
                
            except Exception as e:
                log_exception(logger, e, "worker loop")
                logger.error(f"Error in worker loop: {e}", exc_info=True)
        
        # Finalize any remaining batch before stopping
        if self.batch_start_time is not None:
            self._finalize_batch_changelog()
        
        logger.info("Stream checker worker stopped")
        log_function_return(logger, "_worker_loop")
    
    def _scheduler_loop(self):
        """Scheduler loop for M3U update-triggered and scheduled checks."""
        logger.info("Stream checker scheduler started")
        
        while self.running:
            try:
                # Wait for either a trigger event or timeout (60 seconds for global check monitoring)
                triggered = self.check_trigger.wait(timeout=60)
                
                # Handle trigger for M3U updates
                if triggered:
                    self.check_trigger.clear()
                    # Only process channel queueing if this was a real M3U update trigger
                    # (not a config change wake-up) AND no global action is in progress
                    if not self.config_changed.is_set():
                        if self.global_action_in_progress:
                            logger.info("Skipping channel queueing - global action in progress")
                        else:
                            # Call _queue_updated_channels() directly - it handles pipeline mode checking internally
                            self._queue_updated_channels()
                
                # Check if config was changed
                if self.config_changed.is_set():
                    self.config_changed.clear()
                    logger.info("Configuration change detected, applying new settings immediately")
                
                # Check if it's time for a global check (checked on every iteration)
                # This will set global_action_in_progress if a global action is triggered
                if not self.global_action_in_progress:
                    self._check_global_schedule()
                
            except Exception as e:
                logger.error(f"Error in scheduler loop: {e}", exc_info=True)
        
        logger.info("Stream checker scheduler stopped")
    
    def _queue_updated_channels(self):
        """Queue channels that have received M3U updates.
        
        This respects the pipeline mode:
        - Disabled: Skip all automation
        - Pipeline 1/1.5: Queue channels for checking
        - Pipeline 2/2.5: Skip checking (only update and match)
        - Pipeline 3: Skip checking (only scheduled global actions)
        """
        # Check if auto quality checking is enabled (considers both pipeline mode and individual controls)
        if not self.config.is_auto_quality_checking_enabled():
            logger.info("Skipping channel queueing - automatic quality checking is disabled")
            return
        
        max_channels = self.config.get('queue.max_channels_per_run', 50)
        
        # Atomically get channels and clear their needs_check flag
        # This prevents duplicate queueing if M3U refresh happens during check
        channels_to_queue = self.update_tracker.get_and_clear_channels_needing_check(max_channels)
        
        if channels_to_queue:
            # Remove channels from completed set to allow re-queueing
            # This is necessary when channels receive new streams after being checked
            for channel_id in channels_to_queue:
                self.check_queue.remove_from_completed(channel_id)
            
            added = self.check_queue.add_channels(channels_to_queue, priority=10)
            logger.info(f"Queued {added}/{len(channels_to_queue)} updated channels for checking")
        else:
            logger.debug("No channels need checking")
    
    def _check_global_schedule(self):
        """Check if it's time for a scheduled global action.
        
        Uses cron expression to determine when to run the global action.
        
        On fresh start (no previous check recorded):
        - Only runs if current time is within ±10 minutes of the next scheduled time
        - Otherwise waits for the scheduled time to arrive
        
        On subsequent checks (previous check exists):
        - Runs if the next scheduled time has passed since the last check
        - Prevents duplicate runs by tracking the last check time
        """
        if not self.config.get('global_check_schedule.enabled', True):
            logger.debug("Global check schedule is disabled")
            return
        
        # Check if scheduled global action is enabled (considers both pipeline mode and individual controls)
        if not self.config.is_scheduled_global_action_enabled():
            logger.debug("Skipping global schedule check - scheduled global actions are disabled")
            return
        
        now = datetime.now()
        
        # Get cron expression, with backward compatibility for old config format
        cron_expression = self.config.get('global_check_schedule.cron_expression')
        if not cron_expression:
            # Backward compatibility: convert old format to cron
            cron_expression = self._convert_legacy_schedule_to_cron()
        
        try:
            from croniter import croniter
        except ImportError:
            logger.error("croniter library not installed. Please install it with: pip install croniter")
            return
        
        # Validate cron expression
        if not croniter.is_valid(cron_expression):
            logger.error(f"Invalid cron expression: {cron_expression}")
            return
        
        last_global = self.update_tracker.get_last_global_check()
        
        # Calculate next scheduled time from now
        cron = croniter(cron_expression, now)
        next_scheduled_time = cron.get_next(datetime)
        
        # Calculate previous scheduled time (going back from now)
        cron_prev = croniter(cron_expression, now)
        prev_scheduled_time = cron_prev.get_prev(datetime)
        
        # On fresh start (no previous check), only run if within the scheduled time window (±10 minutes)
        # Otherwise, do nothing and wait for the scheduled time to arrive
        if last_global is None:
            time_diff_minutes = abs((now - prev_scheduled_time).total_seconds() / 60)
            if time_diff_minutes <= 10:
                # We're within the scheduled window on fresh start, run the check
                automation_controls = self.config.get('automation_controls', {})
                logger.info(f"Starting scheduled global action (automation_controls: {automation_controls}, cron: {cron_expression})")
                self._perform_global_action()
                self.update_tracker.mark_global_check()
            else:
                # Fresh start but not within scheduled window, do nothing and wait
                # The scheduler will check again later when the scheduled time arrives
                logger.debug(f"Fresh start outside scheduled window (±10 min of {prev_scheduled_time.strftime('%Y-%m-%d %H:%M')}), waiting for scheduled time")
            return
        
        # Parse last check time
        last_check_time = datetime.fromisoformat(last_global)
        
        # Check if we've passed the previous scheduled time since the last check
        # This prevents running multiple times between scheduled intervals
        if prev_scheduled_time > last_check_time:
            # We've passed a scheduled time since the last check, so we should run
            automation_controls = self.config.get('automation_controls', {})
            logger.info(f"Starting scheduled global action (automation_controls: {automation_controls}, cron: {cron_expression})")
            self._perform_global_action()
            # Mark that global check has been initiated to prevent duplicate queueing
            self.update_tracker.mark_global_check()
    
    def _convert_legacy_schedule_to_cron(self):
        """Convert legacy schedule format (hour/minute/frequency) to cron expression.
        
        This provides backward compatibility for existing configurations.
        """
        frequency = self.config.get('global_check_schedule.frequency', 'daily')
        hour = self.config.get('global_check_schedule.hour', 3)
        minute = self.config.get('global_check_schedule.minute', 0)
        
        if frequency == 'monthly':
            day_of_month = self.config.get('global_check_schedule.day_of_month', 1)
            # Monthly on specific day: minute hour day * *
            cron_expression = f"{minute} {hour} {day_of_month} * *"
        else:
            # Daily: minute hour * * *
            cron_expression = f"{minute} {hour} * * *"
        
        logger.info(f"Converted legacy schedule to cron: {cron_expression}")
        return cron_expression
    
    def _perform_global_action(self):
        """Perform a complete global action: Refresh UDI, Update M3U, Match streams, and Check all channels.
        
        This is the comprehensive global action that:
        1. Refreshes UDI cache to ensure current data from Dispatcharr
        2. Clears ALL dead streams from tracker to give them a second chance
        3. Reloads enabled M3U accounts
        4. Matches new streams with regex patterns (including previously dead ones)
        5. Checks every channel from every stream (bypassing 2-hour immunity)
        6. Disables empty channels if configured
        
        During this operation, regular automated updates, matching, and checking are paused.
        """
        try:
            # Set global action flag to prevent concurrent operations
            self.global_action_in_progress = True
            logger.info("=" * 80)
            logger.info("STARTING GLOBAL ACTION")
            logger.info("Regular automation paused during global action")
            logger.info("=" * 80)
            
            # Step 1: Refresh UDI cache to ensure we have current data from Dispatcharr
            logger.info("Step 1/6: Refreshing UDI cache...")
            try:
                from udi import get_udi_manager
                udi = get_udi_manager()
                refresh_success = udi.refresh_all()
                if refresh_success:
                    logger.info("✓ UDI cache refreshed successfully")
                else:
                    logger.warning("⚠ UDI cache refresh had issues")
            except Exception as e:
                logger.error(f"✗ Failed to refresh UDI cache: {e}")
            
            # Step 2: Clear ALL dead streams from tracker to give them a second chance
            logger.info("Step 2/6: Clearing dead stream tracker to give all streams a second chance...")
            try:
                dead_count = len(self.dead_streams_tracker.get_dead_streams())
                if dead_count > 0:
                    self.dead_streams_tracker.clear_all_dead_streams()
                    logger.info(f"✓ Cleared {dead_count} dead stream(s) from tracker - they will be given a second chance")
                else:
                    logger.info("✓ No dead streams to clear from tracker")
            except Exception as e:
                logger.error(f"✗ Failed to clear dead streams: {e}")
            
            automation_manager = None
            
            # Step 3: Update M3U playlists
            logger.info("Step 3/6: Updating M3U playlists...")
            try:
                from automated_stream_manager import AutomatedStreamManager
                automation_manager = AutomatedStreamManager()
                update_success = automation_manager.refresh_playlists()
                if update_success:
                    logger.info("✓ M3U playlists updated successfully")
                else:
                    logger.warning("⚠ M3U playlist update had issues")
            except Exception as e:
                logger.error(f"✗ Failed to update M3U playlists: {e}")
            
            # Step 4: Validate and remove non-matching streams
            logger.info("Step 4/6: Validating existing streams against regex patterns...")
            try:
                if automation_manager is not None:
                    # Respect automation_controls.remove_non_matching_streams setting
                    validation_results = automation_manager.validate_and_remove_non_matching_streams()
                    if validation_results.get("streams_removed", 0) > 0:
                        logger.info(f"✓ Removed {validation_results['streams_removed']} non-matching streams from {validation_results['channels_modified']} channels")
                    else:
                        logger.info("✓ No non-matching streams found to remove")
                else:
                    logger.warning("⚠ Skipping stream validation - automation manager not available")
            except Exception as e:
                logger.error(f"✗ Failed to validate streams: {e}")
            
            # Step 5: Match and assign streams (including previously dead ones since tracker was cleared)
            logger.info("Step 5/6: Matching and assigning streams...")
            try:
                if automation_manager is not None:
                    assignments = automation_manager.discover_and_assign_streams()
                    if assignments:
                        logger.info(f"✓ Assigned streams to {len(assignments)} channels")
                    else:
                        logger.info("✓ No new stream assignments")
                else:
                    logger.warning("⚠ Skipping stream matching - automation manager not available")
            except Exception as e:
                logger.error(f"✗ Failed to match streams: {e}")
            
            # Step 6: Check all channels (force check to bypass immunity)
            logger.info("Step 6/6: Queueing all channels for checking...")
            self._queue_all_channels(force_check=True)
            
            # Note: Empty channel disabling will be triggered after batch finalization
            
            logger.info("=" * 80)
            logger.info("GLOBAL ACTION INITIATED SUCCESSFULLY")
            logger.info("Regular automation will resume")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"Error performing global action: {e}", exc_info=True)
        finally:
            # Always clear the flag, even if there was an error
            self.global_action_in_progress = False
    
    def _queue_all_channels(self, force_check: bool = False):
        """Queue all channels for checking (global check).
        
        Args:
            force_check: If True, marks channels for force checking which bypasses 2-hour immunity
        """
        try:
            udi = get_udi_manager()
            channels = udi.get_channels()
            
            if channels:
                channel_ids = [ch['id'] for ch in channels if isinstance(ch, dict) and 'id' in ch]
                
                # Filter by profile if one is selected
                profile_config = get_profile_config()
                
                if profile_config.is_using_profile():
                    selected_profile_id = profile_config.get_selected_profile()
                    if selected_profile_id:
                        try:
                            # Get channels that are enabled in this profile from UDI
                            profile_data = udi.get_profile_channels(selected_profile_id)
                            
                            # According to Dispatcharr API, profile_data.channels is a list of channel IDs
                            profile_channel_ids = {
                                ch_id for ch_id in profile_data.get('channels', []) 
                                if isinstance(ch_id, int)
                            }
                            
                            # Filter channels to only those in the profile
                            channels = [ch for ch in channels if ch.get('id') in profile_channel_ids]
                            channel_ids = [ch['id'] for ch in channels]
                            
                            profile_name = profile_config.get_config().get('selected_profile_name', 'Unknown')
                            logger.info(f"Profile filter active: Using {len(channel_ids)} channels from profile '{profile_name}'")
                        except Exception as e:
                            logger.error(f"Failed to load profile channels, using all channels: {e}")
                
                # Filter channels by checking_mode setting (channel-level overrides group-level)
                channel_settings = get_channel_settings_manager()
                filtered_channel_ids = []
                
                for ch in channels:
                    if not isinstance(ch, dict) or 'id' not in ch:
                        continue
                    
                    cid = ch['id']
                    channel_group_id = ch.get('channel_group_id')
                    
                    # Check if channel has an explicit setting (not default)
                    channel_explicit_settings = channel_settings._settings.get(cid, {})
                    has_explicit_checking = 'checking_mode' in channel_explicit_settings
                    
                    if has_explicit_checking:
                        # Channel has explicit override - use it
                        if channel_settings.is_checking_enabled(cid):
                            filtered_channel_ids.append(cid)
                    else:
                        # No channel override - use group setting (or default to enabled if no group)
                        if channel_settings.is_channel_enabled_by_group(channel_group_id, mode='checking'):
                            filtered_channel_ids.append(cid)
                
                excluded_count = len(channel_ids) - len(filtered_channel_ids)
                
                if excluded_count > 0:
                    logger.info(f"Excluding {excluded_count} channel(s) with checking disabled (channel or group level) from global action")
                
                if not filtered_channel_ids:
                    logger.info("No channels with checking enabled to queue for global check")
                    return
                
                if force_check:
                    # Mark all enabled channels for force check (bypasses immunity)
                    for channel_id in filtered_channel_ids:
                        self.update_tracker.mark_channel_for_force_check(channel_id)
                
                # Remove channels from completed set to allow re-queueing
                # This is necessary for global checks to re-check all channels
                for channel_id in filtered_channel_ids:
                    self.check_queue.remove_from_completed(channel_id)
                
                max_channels = self.config.get('queue.max_channels_per_run', 50)
                
                # Queue in batches with higher priority for global checks
                total_added = 0
                for i in range(0, len(filtered_channel_ids), max_channels):
                    batch = filtered_channel_ids[i:i+max_channels]
                    added = self.check_queue.add_channels(batch, priority=5)
                    total_added += added
                
                logger.info(f"Queued {total_added}/{len(filtered_channel_ids)} channels for global check (force_check={force_check})")
        except Exception as e:
            logger.error(f"Failed to queue all channels: {e}")
    
    def _is_stream_dead(self, stream_data: Dict) -> bool:
        """Check if a stream should be considered dead based on analysis results.
        
        Uses centralized utility function for consistent dead stream detection
        with configurable thresholds.
        
        Args:
            stream_data: Analyzed stream data dictionary
            
        Returns:
            bool: True if stream is dead, False otherwise
        """
        # Get dead stream handling configuration
        dead_stream_config = self.config.get('dead_stream_handling', {})
        
        # If dead stream handling is disabled, never consider streams dead
        # (except for the 0x0 resolution and 0 bitrate cases which are always dead)
        if not dead_stream_config.get('enabled', True):
            # Only check for absolute failures (0x0 resolution, 0 bitrate)
            basic_config = {
                'min_resolution_width': 0,
                'min_resolution_height': 0,
                'min_bitrate_kbps': 0,
                'min_score': 0
            }
            return utils_is_stream_dead(stream_data, basic_config)
        
        # Pass the configuration to the utility function
        return utils_is_stream_dead(stream_data, dead_stream_config)
    
    def _calculate_channel_averages(self, analyzed_streams: List[Dict], dead_stream_ids: set) -> Dict[str, str]:
        """Calculate channel-level average statistics from analyzed streams.
        
        Uses centralized utility function for consistent average calculation.
        
        Args:
            analyzed_streams: List of analyzed stream dictionaries
            dead_stream_ids: Set of stream IDs that are marked as dead
            
        Returns:
            Dictionary with avg_resolution, avg_bitrate, and avg_fps
        """
        return calculate_channel_averages(analyzed_streams, dead_stream_ids)
    
    def _get_m3u_account_name(self, stream_id: int, udi=None) -> Optional[str]:
        """Get the M3U account name for a stream.
        
        Args:
            stream_id: The stream ID to look up
            udi: Optional UDI manager instance (will fetch if not provided)
            
        Returns:
            M3U account name or None if not found
        """
        try:
            if udi is None:
                udi = get_udi_manager()
            
            stream_data = udi.get_stream_by_id(stream_id)
            if not stream_data:
                return None
            
            m3u_account_id = stream_data.get('m3u_account')
            if not m3u_account_id:
                return None
            
            m3u_account = udi.get_m3u_account_by_id(m3u_account_id)
            if not m3u_account:
                return None
            
            return m3u_account.get('name', 'Unknown')
        except Exception as e:
            logger.debug(f"Could not fetch M3U account for stream {stream_id}: {e}")
            return None
    
    
    def _update_stream_stats(self, stream_data: Dict) -> bool:
        """Update stream stats for a single stream on the server and sync with UDI cache.
        
        This method:
        1. Constructs the stats payload from analyzed stream data
        2. Merges with existing stats on Dispatcharr
        3. PATCHes the updated stats to Dispatcharr
        4. Updates the UDI cache to keep it in sync
        
        This ensures that the UDI cache always reflects the latest stats written to Dispatcharr,
        preventing inconsistencies between changelog data and actual Dispatcharr data.
        """
        base_url = _get_base_url()
        if not base_url:
            logger.error("DISPATCHARR_BASE_URL not set.")
            return False
        
        stream_id = stream_data.get("stream_id")
        if not stream_id:
            logger.warning("No stream_id in stream data. Skipping stats update.")
            return False
        
        # Construct the stream stats payload from the analyzed stream data
        stream_stats_payload = {
            "resolution": stream_data.get("resolution"),
            "source_fps": stream_data.get("fps"),
            "video_codec": stream_data.get("video_codec"),
            "audio_codec": stream_data.get("audio_codec"),
            "ffmpeg_output_bitrate": int(stream_data.get("bitrate_kbps")) if stream_data.get("bitrate_kbps") not in ["N/A", None] and stream_data.get("bitrate_kbps") else None,
        }
        
        # Clean up the payload, removing any None values or N/A values
        stream_stats_payload = {k: v for k, v in stream_stats_payload.items() if v not in [None, "N/A"]}
        
        if not stream_stats_payload:
            logger.debug(f"No data to update for stream {stream_id}. Skipping.")
            return False
        
        # Construct the URL for the specific stream
        stream_url = f"{base_url}/api/channels/streams/{int(stream_id)}/"
        
        try:
            # Fetch the existing stream data from UDI
            udi = get_udi_manager()
            existing_stream_data = udi.get_stream_by_id(int(stream_id))
            if not existing_stream_data:
                logger.warning(f"Could not fetch existing data for stream {stream_id}. Skipping stats update.")
                return False
            
            # Get the existing stream_stats or an empty dict
            existing_stats = existing_stream_data.get("stream_stats") or {}
            if isinstance(existing_stats, str):
                try:
                    existing_stats = json.loads(existing_stats)
                except json.JSONDecodeError:
                    existing_stats = {}
            
            # Merge the existing stats with the new payload
            updated_stats = {**existing_stats, **stream_stats_payload}
            
            # Send the PATCH request with the updated stream_stats
            patch_payload = {"stream_stats": updated_stats}
            logger.info(f"Updating stream {stream_id} stats with: {stream_stats_payload}")
            patch_request(stream_url, patch_payload)
            
            # Update UDI cache with the new stats to keep it in sync with Dispatcharr
            # This ensures changelog and verification read the correct, up-to-date data
            updated_stream_data = existing_stream_data.copy()
            updated_stream_data['stream_stats'] = updated_stats
            udi.update_stream(int(stream_id), updated_stream_data)
            logger.debug(f"Updated UDI cache for stream {stream_id} with new stats")
            
            return True
        
        except Exception as e:
            logger.error(f"Error updating stats for stream {stream_id}: {e}")
            return False
    
    def _start_batch_changelog(self):
        """Start a new batch for changelog entries."""
        with self.batch_lock:
            self.batch_start_time = datetime.now().isoformat()
            self.batch_changelog_entries = []
            logger.debug("Started new changelog batch")
    
    def _add_to_batch_changelog(self, channel_entry: Dict[str, Any]):
        """Add a channel check result to the current batch.
        
        Args:
            channel_entry: Dictionary containing channel check results
        """
        with self.batch_lock:
            if self.batch_start_time is not None:
                self.batch_changelog_entries.append(channel_entry)
                logger.debug(f"Added channel entry to batch (total: {len(self.batch_changelog_entries)})")
    
    def _finalize_batch_changelog(self):
        """Finalize the current batch and create a consolidated changelog entry."""
        with self.batch_lock:
            if self.batch_start_time is None or len(self.batch_changelog_entries) == 0:
                logger.debug("No batch to finalize")
                return
            
            if not self.changelog:
                logger.debug("Changelog not available, skipping batch finalization")
                self.batch_start_time = None
                self.batch_changelog_entries = []
                return
            
            try:
                # Calculate duration
                start_dt = datetime.fromisoformat(self.batch_start_time)
                end_dt = datetime.now()
                duration_seconds = int((end_dt - start_dt).total_seconds())
                
                # Format duration as human-readable string
                if duration_seconds < 60:
                    duration_str = f"{duration_seconds}s"
                elif duration_seconds < 3600:
                    minutes = duration_seconds // 60
                    seconds = duration_seconds % 60
                    duration_str = f"{minutes}m {seconds}s"
                else:
                    hours = duration_seconds // 3600
                    minutes = (duration_seconds % 3600) // 60
                    duration_str = f"{hours}h {minutes}m"
                
                # Calculate aggregate stats
                total_channels = len(self.batch_changelog_entries)
                total_streams = sum(entry.get('total_streams', 0) for entry in self.batch_changelog_entries)
                streams_analyzed = sum(entry.get('streams_analyzed', 0) for entry in self.batch_changelog_entries)
                dead_streams = sum(entry.get('dead_streams_detected', 0) for entry in self.batch_changelog_entries)
                streams_revived = sum(entry.get('streams_revived', 0) for entry in self.batch_changelog_entries)
                successful_checks = sum(1 for entry in self.batch_changelog_entries if entry.get('success', False))
                failed_checks = total_channels - successful_checks
                
                # Prepare subentries in the format expected by the UI
                subentries = [{
                    "group": "check",
                    "items": [
                        {
                            "channel_id": entry.get('channel_id'),
                            "channel_name": entry.get('channel_name'),
                            "logo_url": entry.get('logo_url'),
                            "stats": {
                                "total_streams": entry.get('total_streams', 0),
                                "streams_analyzed": entry.get('streams_analyzed', 0),
                                "dead_streams": entry.get('dead_streams_detected', 0),
                                "streams_revived": entry.get('streams_revived', 0),
                                "avg_resolution": entry.get('avg_resolution', 'N/A'),
                                "avg_bitrate": entry.get('avg_bitrate', 'N/A'),
                                "avg_fps": entry.get('avg_fps', 'N/A'),
                                "stream_details": entry.get('stream_stats', [])
                            }
                        }
                        for entry in self.batch_changelog_entries
                    ]
                }]
                
                # Create consolidated changelog entry
                self.changelog.add_entry(
                    action='batch_stream_check',
                    details={
                        'total_channels': total_channels,
                        'successful_checks': successful_checks,
                        'failed_checks': failed_checks,
                        'total_streams': total_streams,
                        'streams_analyzed': streams_analyzed,
                        'dead_streams': dead_streams,
                        'streams_revived': streams_revived,
                        'duration': duration_str,
                        'duration_seconds': duration_seconds
                    },
                    timestamp=self.batch_start_time,
                    subentries=subentries
                )
                
                logger.info(f"Finalized batch changelog: {total_channels} channels, {streams_analyzed} streams analyzed in {duration_str}")
                
                # After batch finalization, trigger empty channel disabling if configured
                self._trigger_empty_channel_disabling()
                
            except Exception as e:
                logger.error(f"Failed to finalize batch changelog: {e}", exc_info=True)
            finally:
                # Reset batch tracking
                self.batch_start_time = None
                self.batch_changelog_entries = []
    
    def _trigger_empty_channel_disabling(self):
        """Trigger empty channel disabling if configured.
        
        This method checks if empty channel management is enabled in the profile
        configuration and triggers the disabling operation if so.
        """
        try:
            from empty_channel_manager import trigger_empty_channel_disabling
            
            result = trigger_empty_channel_disabling()
            if result:
                disabled_count, total_checked = result
                if disabled_count > 0:
                    logger.info(f"Empty channel management: Disabled {disabled_count} empty channels (checked {total_checked} channels)")
                else:
                    logger.debug(f"Empty channel management: No empty channels found (checked {total_checked} channels)")
        except Exception as e:
            logger.error(f"Error triggering empty channel disabling: {e}", exc_info=True)
    
    def _check_channel_limits(self, channel_id: int, channel_name: str, streams: List[Dict]) -> Optional[Dict]:
        """Check if a channel can be checked based on viewer and playlist limits.
        
        This method now uses profile-aware checking. Instead of just checking account-level
        max_streams, it verifies that at least one stream has an available profile slot.
        
        Args:
            channel_id: ID of the channel
            channel_name: Name of the channel
            streams: List of streams for the channel
            
        Returns:
            None if check can proceed, or a result dict if check should be skipped
        """
        udi = get_udi_manager()
        
        # Check if channel has active viewers using real-time proxy status
        has_active_viewers = udi.is_channel_active(channel_id)
        if has_active_viewers:
            logger.warning(f"Channel {channel_name} has active viewers, skipping check to avoid disruption")
            return {
                'dead_streams_count': 0,
                'revived_streams_count': 0,
                'skipped': True,
                'skip_reason': 'active_viewers'
            }
        
        # Check if at least one stream can run (has an available profile)
        # This replaces the old account-level checking with profile-aware logic
        has_available_slot = False
        blocked_reasons = []
        
        for stream in streams:
            m3u_account = stream.get('m3u_account')
            if not m3u_account:
                # Custom stream without M3U account - can always check
                has_available_slot = True
                break
            
            # Check if this stream can run using profile-aware checking
            can_run, reason = udi.check_stream_can_run(stream)
            if can_run:
                has_available_slot = True
                break
            else:
                if reason and reason not in blocked_reasons:
                    blocked_reasons.append(reason)
        
        # If no stream has an available slot, skip the check
        if not has_available_slot:
            reason_str = "; ".join(blocked_reasons) if blocked_reasons else "All M3U account profiles are at capacity"
            logger.warning(f"Cannot check channel {channel_name}: {reason_str}")
            return {
                'dead_streams_count': 0,
                'revived_streams_count': 0,
                'skipped': True,
                'skip_reason': 'max_streams_reached',
                'reason_detail': reason_str
            }
        
        # At least one stream has an available slot, check can proceed
        return None
    
    def _check_channel(self, channel_id: int, skip_batch_changelog: bool = False):
        """Check and reorder streams for a specific channel.
        
        Routes to either concurrent or sequential checking based on configuration.
        
        Args:
            channel_id: ID of the channel to check
            skip_batch_changelog: If True, don't add this check to the batch changelog
        """
        concurrent_enabled = self.config.get('concurrent_streams.enabled', True)
        
        if concurrent_enabled:
            return self._check_channel_concurrent(channel_id, skip_batch_changelog=skip_batch_changelog)
        else:
            return self._check_channel_sequential(channel_id, skip_batch_changelog=skip_batch_changelog)
    
    def _check_channel_concurrent(self, channel_id: int, skip_batch_changelog: bool = False):
        """Check and reorder streams for a specific channel using parallel thread pool.
        
        Args:
            channel_id: ID of the channel to check
            skip_batch_changelog: If True, don't add this check to the batch changelog
        """
        import time as time_module
        from stream_check_utils import analyze_stream
        from concurrent_stream_limiter import get_smart_scheduler, get_account_limiter, initialize_account_limits
        
        start_time = time_module.time()
        log_function_call(logger, "_check_channel_concurrent", channel_id=channel_id)
        
        log_state_change(logger, f"channel_{channel_id}", "queued", "checking")
        self.checking = True
        logger.info(f"=" * 80)
        logger.info(f"Checking channel {channel_id} (parallel mode)")
        logger.info(f"=" * 80)
        
        # Get dead stream removal configuration early (used later in finally block)
        dead_stream_removal_enabled = self.config.get('dead_stream_handling', {}).get('enabled', True)
        
        try:
            # Get channel information from UDI
            logger.debug(f"Updating progress for channel {channel_id} initialization")
            self.progress.update(
                channel_id=channel_id,
                channel_name='Loading...',
                current=0,
                total=0,
                status='initializing',
                step='Fetching channel info',
                step_detail='Retrieving channel data from UDI'
            )
            
            udi = get_udi_manager()
            base_url = _get_base_url()
            logger.debug(f"Fetching channel data for channel {channel_id} from UDI")
            channel_data = udi.get_channel_by_id(channel_id)
            if not channel_data:
                logger.error(f"UDI returned None for channel {channel_id}")
                raise Exception(f"Could not fetch channel {channel_id}")
            
            channel_name = channel_data.get('name', f'Channel {channel_id}')
            
            # Get streams for this channel
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=0,
                total=0,
                status='initializing',
                step='Fetching streams',
                step_detail=f'Loading streams for {channel_name}'
            )
            
            streams = fetch_channel_streams(channel_id)
            if not streams or len(streams) == 0:
                logger.info(f"No streams found for channel {channel_name}")
                self.check_queue.mark_completed(channel_id)
                self.update_tracker.mark_channel_checked(channel_id)
                return {
                    'dead_streams_count': 0,
                    'revived_streams_count': 0
                }
            
            logger.info(f"Found {len(streams)} streams for channel {channel_name}")
            
            # Check if channel has active viewers or if its playlist has reached max concurrent streams
            limit_check_result = self._check_channel_limits(channel_id, channel_name, streams)
            if limit_check_result is not None:
                self.check_queue.mark_completed(channel_id)
                self.update_tracker.mark_channel_checked(channel_id)
                return limit_check_result
            
            # Check if this is a force check (bypasses 2-hour immunity)
            force_check = self.update_tracker.should_force_check(channel_id)
            
            # Get list of already checked streams to avoid re-analyzing
            checked_stream_ids = self.update_tracker.get_checked_stream_ids(channel_id)
            current_stream_ids = [s['id'] for s in streams]
            
            # Identify which streams need analysis (new or unchecked)
            if force_check:
                streams_to_check = streams
                streams_already_checked = []
                logger.info(f"Force check enabled: analyzing all {len(streams)} streams (bypassing 2-hour immunity)")
                self.update_tracker.clear_force_check(channel_id)
            else:
                streams_to_check = [s for s in streams if s['id'] not in checked_stream_ids]
                streams_already_checked = [s for s in streams if s['id'] in checked_stream_ids]
                
                if streams_to_check:
                    logger.info(f"Found {len(streams_to_check)} new/unchecked streams (out of {len(streams)} total)")
                else:
                    logger.info(f"All {len(streams)} streams have been recently checked, using cached scores")
                    
                    # Optimization: Skip check entirely if all conditions are met:
                    # 1. No new streams to analyze (all have been checked)
                    # 2. Stream count matches previous check (no additions/deletions)
                    # 3. Set of stream IDs is identical (no stream replacements)
                    previous_stream_count = len(checked_stream_ids)
                    current_stream_count = len(current_stream_ids)
                    
                    if (current_stream_count == previous_stream_count and 
                        set(current_stream_ids) == set(checked_stream_ids)):
                        logger.info(f"Channel {channel_name} unchanged since last check - skipping reorder")
                        self.check_queue.mark_completed(channel_id)
                        # Update timestamp but keep existing checked_stream_ids
                        self.update_tracker.mark_channel_checked(
                            channel_id,
                            stream_count=current_stream_count,
                            checked_stream_ids=checked_stream_ids
                        )
                        return
                    else:
                        logger.info(f"Channel composition changed (prev: {previous_stream_count}, curr: {current_stream_count}) - will reorder")
            
            # Get configuration for analysis
            analysis_params = self.config.get('stream_analysis', {})
            global_limit = self.config.get('concurrent_streams.global_limit', 10)
            stagger_delay = self.config.get('concurrent_streams.stagger_delay', 1.0)
            
            # Initialize account limits from UDI
            accounts = udi.get_m3u_accounts()
            if accounts:
                initialize_account_limits(accounts)
                logger.debug(f"Initialized concurrent stream limits for {len(accounts)} M3U accounts")
            
            # Initialize smart scheduler with account-aware limiting
            smart_scheduler = get_smart_scheduler(global_limit=global_limit)
            
            # Prepare for concurrent execution
            analyzed_streams = []
            dead_stream_ids = set()  # Use set for O(1) lookups
            revived_stream_ids = []
            total_streams = len(streams_to_check)
            completed_count = [0]  # Use list for mutable closure
            
            # Progress callback for parallel checker
            def progress_callback(completed, total, result):
                completed_count[0] = completed
                stream_name = result.get('stream_name', 'Unknown')
                
                # DO NOT update stream stats here - wait until all checks complete
                # This prevents race conditions with concurrent checks
                
                # Update progress
                self.progress.update(
                    channel_id=channel_id,
                    channel_name=channel_name,
                    current=completed,
                    total=total,
                    current_stream=stream_name,
                    status='analyzing',
                    step='Analyzing streams with account limits',
                    step_detail=f'Completed {completed}/{total}'
                )
            
            if streams_to_check:
                logger.info(f"Starting smart parallel analysis of {total_streams} streams with {global_limit} global workers")
                
                self.progress.update(
                    channel_id=channel_id,
                    channel_name=channel_name,
                    current=0,
                    total=total_streams,
                    status='analyzing',
                    step='Analyzing streams with account limits',
                    step_detail=f'Using smart scheduler with per-account limits'
                )
                
                # Create a wrapper function that uses profile failover
                def analyze_stream_with_profile_failover_wrapper(stream_url, stream_id, stream_name, **kwargs):
                    """Wrapper function that uses profile failover for concurrent checking.
                    
                    Note: stream_url parameter is ignored - we get the stream from UDI and let
                    profile failover handle URL transformation per profile.
                    """
                    # Get full stream data from UDI
                    stream = udi.get_stream_by_id(stream_id)
                    if not stream:
                        logger.error(f"Stream {stream_id} not found in UDI")
                        return {
                            'stream_id': stream_id,
                            'stream_name': stream_name,
                            'stream_url': stream_url,
                            'status': 'Error',
                            'error': 'Stream not found in UDI'
                        }
                    
                    # Use profile failover analysis (Phase 1 + Phase 2)
                    return self._analyze_stream_with_profile_failover(
                        stream=stream,
                        analysis_params=analysis_params,
                        udi=udi
                    )
                
                # Check streams in parallel with account-aware limits and profile failover
                results = smart_scheduler.check_streams_with_limits(
                    streams=streams_to_check,
                    check_function=analyze_stream_with_profile_failover_wrapper,
                    progress_callback=progress_callback,
                    stagger_delay=stagger_delay
                )
                
                # Process results - ALL checks are complete at this point
                # This is the correct place to update stats and track dead streams
                for analyzed in results:
                    # Update stream stats on Dispatcharr with ffmpeg-extracted data
                    # Now that all parallel checks are complete, we can safely push the info
                    self._update_stream_stats(analyzed)
                    
                    # Check if stream is dead
                    is_dead = self._is_stream_dead(analyzed)
                    stream_id = analyzed.get('stream_id')
                    stream_url = analyzed.get('stream_url', '')
                    stream_name = analyzed.get('stream_name', 'Unknown')
                    was_dead = self.dead_streams_tracker.is_dead(stream_url)
                    
                    if is_dead and not was_dead:
                        if self.dead_streams_tracker.mark_as_dead(stream_url, stream_id, stream_name, channel_id):
                            dead_stream_ids.add(stream_id)
                            logger.warning(f"Stream {stream_id} detected as DEAD: {stream_name}")
                        else:
                            logger.error(f"Failed to mark stream {stream_id} as dead in tracker")
                    elif not is_dead and was_dead:
                        if self.dead_streams_tracker.mark_as_alive(stream_url):
                            revived_stream_ids.append(stream_id)
                            logger.info(f"Stream {stream_id} REVIVED: {stream_name}")
                    elif is_dead and was_dead:
                        logger.debug(f"Stream {stream_id} remains dead (already marked)")
                        # Add to dead_stream_ids so the stream removal logic (line 1455) will filter it out
                        dead_stream_ids.add(stream_id)
                    
                    # Calculate score
                    score = self._calculate_stream_score(analyzed, channel_id)
                    analyzed['score'] = score
                    analyzed['channel_id'] = channel_id
                    analyzed['channel_name'] = channel_name
                    analyzed_streams.append(analyzed)
                
                logger.info(f"Completed smart parallel analysis of {len(results)} streams with account-aware limits")
            
            # Process already-checked streams (use cached data)
            for stream in streams_already_checked:
                stream_data = udi.get_stream_by_id(stream['id'])
                if stream_data:
                    stream_stats = stream_data.get('stream_stats', {})
                    if stream_stats is None:
                        stream_stats = {}
                    if isinstance(stream_stats, str):
                        try:
                            stream_stats = json.loads(stream_stats)
                            if stream_stats is None:
                                stream_stats = {}
                        except json.JSONDecodeError:
                            stream_stats = {}
                    
                    analyzed = {
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'stream_id': stream['id'],
                        'stream_name': stream.get('name', 'Unknown'),
                        'stream_url': stream.get('url', ''),
                        'resolution': stream_stats.get('resolution', '0x0'),
                        'fps': stream_stats.get('source_fps', 0),
                        'video_codec': stream_stats.get('video_codec', 'N/A'),
                        'audio_codec': stream_stats.get('audio_codec', 'N/A'),
                        'bitrate_kbps': stream_stats.get('ffmpeg_output_bitrate', 0),
                        'status': 'OK'
                    }
                    
                    # Check if cached stream is dead
                    stream_url = stream.get('url', '')
                    stream_name = stream.get('name', 'Unknown')
                    is_dead = self._is_stream_dead(analyzed)
                    was_dead = self.dead_streams_tracker.is_dead(stream_url)
                    
                    # Handle dead/alive state transitions (same logic as newly-checked streams)
                    if is_dead and not was_dead:
                        # Newly detected as dead
                        if self.dead_streams_tracker.mark_as_dead(stream_url, stream['id'], stream_name, channel_id):
                            dead_stream_ids.add(stream['id'])
                            logger.warning(f"Cached stream {stream['id']} detected as DEAD: {stream_name}")
                        else:
                            logger.error(f"Failed to mark cached stream {stream['id']} as dead in tracker")
                    elif not is_dead and was_dead:
                        # Stream was revived!
                        if self.dead_streams_tracker.mark_as_alive(stream_url):
                            revived_stream_ids.append(stream['id'])
                            logger.info(f"Cached stream {stream['id']} REVIVED: {stream_name}")
                        else:
                            logger.error(f"Failed to mark cached stream {stream['id']} as alive")
                    elif is_dead and was_dead:
                        # Stream remains dead (already marked)
                        logger.debug(f"Cached stream {stream['id']} remains dead (already marked)")
                        dead_stream_ids.add(stream['id'])
                    
                    score = self._calculate_stream_score(analyzed, channel_id)
                    analyzed['score'] = score
                    analyzed_streams.append(analyzed)
            
            # Sort streams by score (highest first)
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=len(streams),
                total=len(streams),
                status='processing',
                step='Calculating scores',
                step_detail='Sorting streams by quality score'
            )
            analyzed_streams.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # Apply provider diversification if enabled
            if self.config.get('stream_ordering', {}).get('provider_diversification', False):
                analyzed_streams = self._apply_provider_diversification(analyzed_streams, channel_id)
            
            # Remove dead streams from the channel (if enabled in config)
            # Dead streams are checked during all channel checks (normal and global)
            # If they're still dead, they're removed; if revived, they remain
            if dead_stream_ids:
                if dead_stream_removal_enabled:
                    logger.warning(f"🔴 Removing {len(dead_stream_ids)} dead streams from channel {channel_name}")
                    analyzed_streams = [s for s in analyzed_streams if s.get('stream_id') not in dead_stream_ids]
                else:
                    logger.info(f"⚠️ Found {len(dead_stream_ids)} dead streams in channel {channel_name}, but removal is disabled in config")
            
            if revived_stream_ids:
                logger.info(f"{len(revived_stream_ids)} streams were revived in channel {channel_name}")
            
            # Update channel with reordered streams
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=len(streams),
                total=len(streams),
                status='updating',
                step='Reordering streams',
                step_detail='Applying new stream order to channel'
            )
            reordered_ids = [s.get('stream_id') for s in analyzed_streams if s.get('stream_id') is not None]
            # Dead streams have already been filtered from analyzed_streams if removal is enabled
            # If removal is disabled, allow them to remain in the channel
            update_channel_streams(channel_id, reordered_ids, allow_dead_streams=(not dead_stream_removal_enabled))
            
            # Verify the update
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=len(streams),
                total=len(streams),
                status='verifying',
                step='Verifying update',
                step_detail='Confirming stream order was applied'
            )
            time_module.sleep(0.5)
            udi.refresh_channel_by_id(channel_id)
            
            logger.info(f"✓ Channel {channel_name} checked and streams reordered (parallel mode)")
            
            # Add to batch changelog instead of creating individual entry
            if self.changelog:
                try:
                    # Get channel logo URL
                    logo_url = None
                    logo_id = channel_data.get('logo_id')
                    if logo_id:
                        logo = udi.get_logo_by_id(logo_id)
                        if logo:
                            logo_url = logo.get('cache_url') or logo.get('url')
                    
                    # Calculate channel-level averages from analyzed streams
                    averages = self._calculate_channel_averages(analyzed_streams, dead_stream_ids)
                    
                    stream_stats = []
                    for analyzed in analyzed_streams[:10]:  # Limit to first 10
                        stream_id = analyzed.get('stream_id')
                        is_dead = stream_id in dead_stream_ids
                        is_revived = stream_id in revived_stream_ids
                        
                        # Extract and format stats using centralized utilities
                        extracted_stats = extract_stream_stats(analyzed)
                        formatted_stats = format_stream_stats_for_display(extracted_stats)
                        
                        # Get M3U account name for this stream using helper method
                        m3u_account_name = self._get_m3u_account_name(stream_id, udi)
                        
                        stream_stat = {
                            'stream_id': stream_id,
                            'stream_name': analyzed.get('stream_name'),
                            'resolution': formatted_stats['resolution'],
                            'fps': formatted_stats['fps'],
                            'video_codec': formatted_stats['video_codec'],
                            'bitrate': formatted_stats['bitrate'],
                            'm3u_account': m3u_account_name
                        }
                        
                        # Mark dead streams as "dead" instead of showing score:0
                        if is_dead:
                            stream_stat['status'] = 'dead'
                        elif is_revived:
                            stream_stat['status'] = 'revived'
                            stream_stat['score'] = round(analyzed.get('score', 0), 2)
                        else:
                            stream_stat['score'] = round(analyzed.get('score', 0), 2)
                        
                        stream_stats.append({k: v for k, v in stream_stat.items() if v not in [None, "N/A"]})
                    
                    # Add to batch instead of creating individual changelog entry
                    # Only add to batch if not explicitly skipped (e.g., when called from check_single_channel)
                    if not skip_batch_changelog:
                        self._add_to_batch_changelog({
                            'channel_id': channel_id,
                            'channel_name': channel_name,
                            'logo_url': logo_url,
                            'total_streams': len(streams),
                            'streams_analyzed': len(analyzed_streams),
                            'dead_streams_detected': len(dead_stream_ids),
                            'streams_revived': len(revived_stream_ids),
                            'avg_resolution': averages['avg_resolution'],
                            'avg_bitrate': averages['avg_bitrate'],
                            'avg_fps': averages['avg_fps'],
                            'success': True,
                            'stream_stats': stream_stats
                        })
                except Exception as e:
                    logger.warning(f"Failed to add to batch changelog: {e}")
            
            # Mark as completed
            self.check_queue.mark_completed(channel_id)
            # Update current_stream_ids to exclude dead streams that were removed
            # This prevents dead stream IDs from being saved in checked_stream_ids
            # which would cause them to be skipped by 2-hour immunity even after revival
            # Note: Using list comprehension instead of set operations to preserve order
            # Only exclude dead streams if removal is enabled
            if dead_stream_removal_enabled:
                final_stream_ids = [sid for sid in current_stream_ids if sid not in dead_stream_ids]
            else:
                final_stream_ids = current_stream_ids  # Keep all streams if removal is disabled
            self.update_tracker.mark_channel_checked(
                channel_id, 
                stream_count=len(streams),
                checked_stream_ids=final_stream_ids
            )
            
            # Return statistics for callers that need them
            return {
                'dead_streams_count': len(dead_stream_ids),
                'revived_streams_count': len(revived_stream_ids)
            }
            
        except Exception as e:
            logger.error(f"Error checking channel {channel_id}: {e}", exc_info=True)
            self.check_queue.mark_failed(channel_id, str(e))
            
            # Only add to batch changelog if not explicitly skipped
            if self.changelog and not skip_batch_changelog:
                try:
                    try:
                        channel_name = channel_data.get('name', f'Channel {channel_id}')
                    except:
                        channel_name = f'Channel {channel_id}'
                    
                    # Add failed check to batch
                    self._add_to_batch_changelog({
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'total_streams': 0,
                        'streams_analyzed': 0,
                        'dead_streams_detected': 0,
                        'streams_revived': 0,
                        'success': False,
                        'error': str(e),
                        'stream_stats': []
                    })
                except Exception as changelog_error:
                    logger.warning(f"Failed to add to batch changelog: {changelog_error}")
            
            # Return empty stats on error
            return {
                'dead_streams_count': 0,
                'revived_streams_count': 0
            }
        
        finally:
            self.checking = False
            self.progress.clear()
            log_function_return(logger, "_check_channel_concurrent")

    
    def _check_channel_sequential(self, channel_id: int, skip_batch_changelog: bool = False):
        """Check and reorder streams for a specific channel using sequential checking.
        
        Args:
            channel_id: ID of the channel to check
            skip_batch_changelog: If True, don't add this check to the batch changelog
        """
        import time as time_module
        start_time = time_module.time()
        log_function_call(logger, "_check_channel_sequential", channel_id=channel_id)
        
        log_state_change(logger, f"channel_{channel_id}", "queued", "checking")
        self.checking = True
        logger.info(f"=" * 80)
        logger.info(f"Checking channel {channel_id} (sequential mode)")
        logger.info(f"=" * 80)
        
        # Get dead stream removal configuration early (used later in finally block)
        dead_stream_removal_enabled = self.config.get('dead_stream_handling', {}).get('enabled', True)
        
        try:
            # Get channel information from UDI
            logger.debug(f"Updating progress for channel {channel_id} initialization")
            self.progress.update(
                channel_id=channel_id,
                channel_name='Loading...',
                current=0,
                total=0,
                status='initializing',
                step='Fetching channel info',
                step_detail='Retrieving channel data from UDI'
            )
            
            udi = get_udi_manager()
            base_url = _get_base_url()
            logger.debug(f"Fetching channel data for channel {channel_id} from UDI")
            channel_data = udi.get_channel_by_id(channel_id)
            if not channel_data:
                logger.error(f"UDI returned None for channel {channel_id}")
                raise Exception(f"Could not fetch channel {channel_id}")
            
            channel_name = channel_data.get('name', f'Channel {channel_id}')
            
            # Get streams for this channel
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=0,
                total=0,
                status='initializing',
                step='Fetching streams',
                step_detail=f'Loading streams for {channel_name}'
            )
            
            streams = fetch_channel_streams(channel_id)
            if not streams or len(streams) == 0:
                logger.info(f"No streams found for channel {channel_name}")
                self.check_queue.mark_completed(channel_id)
                self.update_tracker.mark_channel_checked(channel_id)
                return {
                    'dead_streams_count': 0,
                    'revived_streams_count': 0
                }
            
            logger.info(f"Found {len(streams)} streams for channel {channel_name}")
            
            # Check if channel has active viewers or if its playlist has reached max concurrent streams
            limit_check_result = self._check_channel_limits(channel_id, channel_name, streams)
            if limit_check_result is not None:
                self.check_queue.mark_completed(channel_id)
                self.update_tracker.mark_channel_checked(channel_id)
                return limit_check_result
            
            # Check if this is a force check (bypasses 2-hour immunity)
            force_check = self.update_tracker.should_force_check(channel_id)
            
            # Get list of already checked streams to avoid re-analyzing
            checked_stream_ids = self.update_tracker.get_checked_stream_ids(channel_id)
            current_stream_ids = [s['id'] for s in streams]
            
            # Identify which streams need analysis (new or unchecked)
            # If force_check is True, check ALL streams regardless of immunity
            if force_check:
                streams_to_check = streams
                streams_already_checked = []
                logger.info(f"Force check enabled: analyzing all {len(streams)} streams (bypassing 2-hour immunity)")
                # Clear the force check flag after acknowledging it
                self.update_tracker.clear_force_check(channel_id)
            else:
                streams_to_check = [s for s in streams if s['id'] not in checked_stream_ids]
                streams_already_checked = [s for s in streams if s['id'] in checked_stream_ids]
                
                if streams_to_check:
                    logger.info(f"Found {len(streams_to_check)} new/unchecked streams (out of {len(streams)} total)")
                else:
                    logger.info(f"All {len(streams)} streams have been recently checked, using cached scores")
                    
                    # Optimization: Skip check entirely if all conditions are met:
                    # 1. No new streams to analyze (all have been checked)
                    # 2. Stream count matches previous check (no additions/deletions)
                    # 3. Set of stream IDs is identical (no stream replacements)
                    previous_stream_count = len(checked_stream_ids)
                    current_stream_count = len(current_stream_ids)
                    
                    if (current_stream_count == previous_stream_count and 
                        set(current_stream_ids) == set(checked_stream_ids)):
                        logger.info(f"Channel {channel_name} unchanged since last check - skipping reorder")
                        self.check_queue.mark_completed(channel_id)
                        # Update timestamp but keep existing checked_stream_ids
                        self.update_tracker.mark_channel_checked(
                            channel_id,
                            stream_count=current_stream_count,
                            checked_stream_ids=checked_stream_ids
                        )
                        return
                    else:
                        logger.info(f"Channel composition changed (prev: {previous_stream_count}, curr: {current_stream_count}) - will reorder")
            
            # Import stream analysis functions from stream_check_utils
            from stream_check_utils import analyze_stream
            
            # Analyze new/unchecked streams
            analyzed_streams = []
            dead_stream_ids = set()  # Use set for O(1) lookups
            revived_stream_ids = []
            total_streams = len(streams_to_check)
            
            for idx, stream in enumerate(streams_to_check, 1):
                self.progress.update(
                    channel_id=channel_id,
                    channel_name=channel_name,
                    current=idx,
                    total=total_streams,
                    current_stream=stream.get('name', 'Unknown'),
                    status='analyzing',
                    step='Analyzing stream quality',
                    step_detail=f'Checking bitrate, resolution, codec ({idx}/{total_streams})'
                )
                
                # Analyze stream with profile failover
                analysis_params = self.config.get('stream_analysis', {})
                analyzed = self._analyze_stream_with_profile_failover(
                    stream=stream,
                    analysis_params=analysis_params,
                    udi=udi
                )
                
                # Update stream stats on dispatcharr with ffmpeg-extracted data
                self._update_stream_stats(analyzed)
                
                # Check if stream is dead (resolution=0 or bitrate=0)
                is_dead = self._is_stream_dead(analyzed)
                stream_url = stream.get('url', '')
                stream_name = stream.get('name', 'Unknown')
                was_dead = self.dead_streams_tracker.is_dead(stream_url)
                
                if is_dead and not was_dead:
                    # Mark as dead in tracker
                    if self.dead_streams_tracker.mark_as_dead(stream_url, stream['id'], stream_name, channel_id):
                        dead_stream_ids.add(stream['id'])
                        logger.warning(f"Stream {stream['id']} detected as DEAD: {stream_name}")
                    else:
                        logger.error(f"Failed to mark stream {stream['id']} as DEAD, will not remove from channel")
                elif not is_dead and was_dead:
                    # Stream was revived!
                    if self.dead_streams_tracker.mark_as_alive(stream_url):
                        revived_stream_ids.append(stream['id'])
                        logger.info(f"Stream {stream['id']} REVIVED: {stream_name}")
                    else:
                        logger.error(f"Failed to mark stream {stream['id']} as alive")
                elif is_dead and was_dead:
                    # Stream remains dead
                    dead_stream_ids.add(stream['id'])
                
                # Calculate score
                score = self._calculate_stream_score(analyzed, channel_id)
                analyzed['score'] = score
                analyzed_streams.append(analyzed)
                
                logger.info(f"Stream {idx}/{total_streams}: {stream.get('name')} - Score: {score:.2f}")
            
            # For already-checked streams, retrieve their cached data from UDI
            for stream in streams_already_checked:
                stream_data = udi.get_stream_by_id(stream['id'])
                if stream_data:
                    stream_stats = stream_data.get('stream_stats', {})
                    # Handle None case explicitly
                    if stream_stats is None:
                        stream_stats = {}
                    if isinstance(stream_stats, str):
                        try:
                            stream_stats = json.loads(stream_stats)
                            # Handle case where JSON string is "null"
                            if stream_stats is None:
                                stream_stats = {}
                        except json.JSONDecodeError:
                            stream_stats = {}
                    
                    # Reconstruct analyzed format from stored stats
                    # Use "0x0" for resolution, 0 for FPS and bitrate when not available
                    analyzed = {
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'stream_id': stream['id'],
                        'stream_name': stream.get('name', 'Unknown'),
                        'stream_url': stream.get('url', ''),
                        'resolution': stream_stats.get('resolution', '0x0'),
                        'fps': stream_stats.get('source_fps', 0),
                        'video_codec': stream_stats.get('video_codec', 'N/A'),
                        'audio_codec': stream_stats.get('audio_codec', 'N/A'),
                        'bitrate_kbps': stream_stats.get('ffmpeg_output_bitrate', 0),
                        'status': 'OK'  # Assume OK for previously checked streams
                    }
                    
                    # Check if this cached stream is dead and handle state transitions
                    stream_url = stream.get('url', '')
                    stream_name = stream.get('name', 'Unknown')
                    is_dead = self._is_stream_dead(analyzed)
                    was_dead = self.dead_streams_tracker.is_dead(stream_url)
                    
                    # Handle dead/alive state transitions (same logic as newly-checked streams)
                    if is_dead and not was_dead:
                        # Newly detected as dead
                        if self.dead_streams_tracker.mark_as_dead(stream_url, stream['id'], stream_name, channel_id):
                            dead_stream_ids.add(stream['id'])
                            logger.warning(f"Cached stream {stream['id']} detected as DEAD: {stream_name}")
                        else:
                            logger.error(f"Failed to mark cached stream {stream['id']} as DEAD, will not remove from channel")
                    elif not is_dead and was_dead:
                        # Stream was revived!
                        if self.dead_streams_tracker.mark_as_alive(stream_url):
                            revived_stream_ids.append(stream['id'])
                            logger.info(f"Cached stream {stream['id']} REVIVED: {stream_name}")
                        else:
                            logger.error(f"Failed to mark cached stream {stream['id']} as alive")
                    elif is_dead and was_dead:
                        # Stream remains dead (already marked)
                        logger.debug(f"Cached stream {stream['id']} remains dead (already marked)")
                        dead_stream_ids.add(stream['id'])
                    
                    # Recalculate score from cached data
                    score = self._calculate_stream_score(analyzed, channel_id)
                    analyzed['score'] = score
                    analyzed_streams.append(analyzed)
                    logger.debug(f"Using cached data for stream {stream['id']}: {stream.get('name')} - Score: {score:.2f}")
                else:
                    # If we can't fetch cached data, analyze this stream
                    logger.warning(f"Could not fetch cached data for stream {stream['id']}, will analyze")
                    analysis_params = self.config.get('stream_analysis', {})
                    
                    # Analyze stream with profile failover
                    analyzed = self._analyze_stream_with_profile_failover(
                        stream=stream,
                        analysis_params=analysis_params,
                        udi=udi
                    )
                    self._update_stream_stats(analyzed)
                    score = self._calculate_stream_score(analyzed, channel_id)
                    analyzed['score'] = score
                    analyzed_streams.append(analyzed)
            
            # Sort streams by score (highest first)
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=len(streams),
                total=len(streams),
                status='processing',
                step='Calculating scores',
                step_detail='Sorting streams by quality score'
            )
            analyzed_streams.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # Apply provider diversification if enabled
            if self.config.get('stream_ordering', {}).get('provider_diversification', False):
                analyzed_streams = self._apply_provider_diversification(analyzed_streams, channel_id)
            
            # Remove dead streams from the channel (if enabled in config)
            # Dead streams are checked during all channel checks (normal and global)
            # If they're still dead, they're removed; if revived, they remain
            if dead_stream_ids:
                if dead_stream_removal_enabled:
                    logger.warning(f"🔴 Removing {len(dead_stream_ids)} dead streams from channel {channel_name}")
                    # Log which streams are being removed
                    for stream_id in dead_stream_ids:
                        dead_stream = next((s for s in analyzed_streams if s.get('stream_id') == stream_id), None)
                        if dead_stream:
                            logger.info(f"  - Removing dead stream {stream_id}: {dead_stream.get('stream_name', 'Unknown')}")
                    analyzed_streams = [s for s in analyzed_streams if s.get('stream_id') not in dead_stream_ids]
                else:
                    logger.info(f"⚠️ Found {len(dead_stream_ids)} dead streams in channel {channel_name}, but removal is disabled in config")
            
            if revived_stream_ids:
                logger.info(f"{len(revived_stream_ids)} streams were revived in channel {channel_name}")
            
            # Update channel with reordered streams
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=len(streams),
                total=len(streams),
                status='updating',
                step='Reordering streams',
                step_detail='Applying new stream order to channel'
            )
            reordered_ids = [s.get('stream_id') for s in analyzed_streams if s.get('stream_id') is not None]
            # Dead streams have already been filtered from analyzed_streams if removal is enabled
            # If removal is disabled, allow them to remain in the channel
            update_channel_streams(channel_id, reordered_ids, allow_dead_streams=(not dead_stream_removal_enabled))
            
            # Verify the update was applied correctly
            self.progress.update(
                channel_id=channel_id,
                channel_name=channel_name,
                current=len(streams),
                total=len(streams),
                status='verifying',
                step='Verifying update',
                step_detail='Confirming stream order was applied'
            )
            time.sleep(0.5)  # Brief delay to ensure API has processed the update
            # Refresh this specific channel in UDI to get updated data after write
            udi.refresh_channel_by_id(channel_id)
            updated_channel_data = udi.get_channel_by_id(channel_id)
            if updated_channel_data:
                updated_stream_ids = updated_channel_data.get('streams', [])
                if updated_stream_ids == reordered_ids:
                    logger.info(f"✓ Verified: Channel {channel_name} streams reordered correctly")
                else:
                    logger.warning(f"⚠ Verification failed: Stream order mismatch for channel {channel_name}")
                    logger.warning(f"Expected: {reordered_ids[:5]}... Got: {updated_stream_ids[:5]}...")
            else:
                logger.warning(f"⚠ Could not verify stream update for channel {channel_name}")
            
            logger.info(f"✓ Channel {channel_name} checked and streams reordered")
            
            # Add changelog entry with stream stats
            if self.changelog:
                try:
                    # Calculate channel-level averages from analyzed streams
                    averages = self._calculate_channel_averages(analyzed_streams, dead_stream_ids)
                    
                    # Prepare stream stats summary for changelog
                    stream_stats = []
                    for analyzed in analyzed_streams[:10]:  # Limit to top 10
                        stream_id = analyzed.get('stream_id')
                        is_dead = stream_id in dead_stream_ids
                        is_revived = stream_id in revived_stream_ids
                        
                        # Extract and format stats using centralized utilities
                        extracted_stats = extract_stream_stats(analyzed)
                        formatted_stats = format_stream_stats_for_display(extracted_stats)
                        
                        # Get M3U account name for this stream using helper method
                        m3u_account_name = self._get_m3u_account_name(stream_id, udi)
                        
                        stream_stat = {
                            'stream_id': stream_id,
                            'stream_name': analyzed.get('stream_name'),
                            'resolution': formatted_stats['resolution'],
                            'fps': formatted_stats['fps'],
                            'video_codec': formatted_stats['video_codec'],
                            'audio_codec': formatted_stats['audio_codec'],
                            'bitrate': formatted_stats['bitrate'],
                            'm3u_account': m3u_account_name
                        }
                        
                        # Mark dead streams as "dead" instead of showing score:0
                        if is_dead:
                            stream_stat['status'] = 'dead'
                        elif is_revived:
                            stream_stat['status'] = 'revived'
                            stream_stat['score'] = round(analyzed.get('score', 0), 2)
                        else:
                            stream_stat['score'] = round(analyzed.get('score', 0), 2)
                            # Include original status from analysis if present
                            if 'status' in analyzed:
                                stream_stat['analysis_status'] = analyzed.get('status')
                        
                        # Clean up N/A values for cleaner output
                        stream_stat = {k: v for k, v in stream_stat.items() if v not in [None, "N/A"]}
                        stream_stats.append(stream_stat)
                    
                    # Get channel logo URL
                    logo_url = None
                    logo_id = channel_data.get('logo_id')
                    if logo_id:
                        logo = udi.get_logo_by_id(logo_id)
                        if logo:
                            logo_url = logo.get('cache_url') or logo.get('url')
                    
                    # Add to batch changelog instead of creating individual entry
                    # Only add to batch if not explicitly skipped (e.g., when called from check_single_channel)
                    if not skip_batch_changelog:
                        self._add_to_batch_changelog({
                            'channel_id': channel_id,
                            'channel_name': channel_name,
                            'logo_url': logo_url,
                            'total_streams': len(streams),
                            'streams_analyzed': len(analyzed_streams),
                            'dead_streams_detected': len(dead_stream_ids),
                            'streams_revived': len(revived_stream_ids),
                            'avg_resolution': averages['avg_resolution'],
                            'avg_bitrate': averages['avg_bitrate'],
                            'avg_fps': averages['avg_fps'],
                            'success': True,
                            'stream_stats': stream_stats[:10]  # Limit to top 10 for brevity
                        })
                        logger.info(f"Added channel {channel_name} to batch changelog")
                except Exception as e:
                    logger.warning(f"Failed to add to batch changelog: {e}")
            
            # Mark as completed with stream count and checked stream IDs
            self.check_queue.mark_completed(channel_id)
            # Update current_stream_ids to exclude dead streams that were removed
            # This prevents dead stream IDs from being saved in checked_stream_ids
            # which would cause them to be skipped by 2-hour immunity even after revival
            # Note: Using list comprehension instead of set operations to preserve order
            # Only exclude dead streams if removal is enabled
            if dead_stream_removal_enabled:
                final_stream_ids = [sid for sid in current_stream_ids if sid not in dead_stream_ids]
            else:
                final_stream_ids = current_stream_ids  # Keep all streams if removal is disabled
            self.update_tracker.mark_channel_checked(
                channel_id, 
                stream_count=len(streams),
                checked_stream_ids=final_stream_ids
            )
            
            # Return statistics for callers that need them
            return {
                'dead_streams_count': len(dead_stream_ids),
                'revived_streams_count': len(revived_stream_ids)
            }
            
        except Exception as e:
            logger.error(f"Error checking channel {channel_id}: {e}", exc_info=True)
            self.check_queue.mark_failed(channel_id, str(e))
            
            # Add failed check to batch changelog
            # Only add to batch if not explicitly skipped
            if self.changelog and not skip_batch_changelog:
                try:
                    # Try to get channel name if available
                    try:
                        channel_name = channel_data.get('name', f'Channel {channel_id}')
                    except:
                        channel_name = f'Channel {channel_id}'
                    
                    self._add_to_batch_changelog({
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'total_streams': 0,
                        'streams_analyzed': 0,
                        'dead_streams_detected': 0,
                        'streams_revived': 0,
                        'success': False,
                        'error': str(e),
                        'stream_stats': []
                    })
                except Exception as changelog_error:
                    logger.warning(f"Failed to add to batch changelog: {changelog_error}")
            
            # Return empty stats on error
            return {
                'dead_streams_count': 0,
                'revived_streams_count': 0
            }
        
        finally:
            self.checking = False
            self.progress.clear()
    
    def _analyze_stream_with_profile_failover(self, stream: Dict, analysis_params: Dict, udi) -> Dict:
        """Analyze a stream with automatic profile failover.
        
        If a stream fails with one profile, automatically tries other available profiles
        before marking the stream as dead. This significantly improves reliability when
        one profile has issues but others work fine.
        
        Strategy:
        1. Try all immediately available profiles (with free slots)
        2. If all fail, try ALL profiles (including full ones) - will wait for slots
        3. Only mark as dead if ALL profiles fail
        
        Args:
            stream: Stream dictionary with 'id', 'name', 'url', etc.
            analysis_params: Analysis parameters from config
            udi: UDI manager instance
            
        Returns:
            Analysis result dictionary
        """
        from stream_check_utils import analyze_stream
        from api_utils import get_stream_proxy
        
        stream_id = stream['id']
        stream_name = stream.get('name', 'Unknown')
        
        # Phase 1: Try available profiles (with free slots)
        available_profiles = []
        if udi and stream.get('m3u_account'):
            available_profiles = udi.get_all_available_profiles_for_stream(stream)
        
        # If no profiles or custom stream, use standard analysis
        if not available_profiles and not stream.get('m3u_account'):
            stream_url = stream.get('url', '')
            if udi:
                stream_url = udi.apply_profile_url_transformation(stream)
            
            proxy = get_stream_proxy(stream_id)
            
            return analyze_stream(
                stream_url=stream_url,
                stream_id=stream_id,
                stream_name=stream_name,
                ffmpeg_duration=analysis_params.get('ffmpeg_duration', 20),
                timeout=analysis_params.get('timeout', 30),
                retries=analysis_params.get('retries', 1),
                retry_delay=analysis_params.get('retry_delay', 10),
                user_agent=analysis_params.get('user_agent', 'VLC/3.0.14'),
                stream_startup_buffer=analysis_params.get('stream_startup_buffer', 10),
                proxy=proxy
            )
        
        # Try available profiles first
        logger.info(f"Stream {stream_id} ({stream_name}): Phase 1 - Trying {len(available_profiles)} available profile(s)")
        
        last_error = None
        for profile_idx, profile in enumerate(available_profiles, 1):
            profile_id = profile.get('id')
            profile_name = profile.get('name', f'Profile {profile_id}')
            
            try:
                # Apply URL transformation for this specific profile
                stream_url = udi.apply_profile_url_transformation(stream, profile)
                
                logger.info(f"Stream {stream_id}: Trying available profile {profile_idx}/{len(available_profiles)} - {profile_name} (ID: {profile_id})")
                
                proxy = get_stream_proxy(stream_id)
                
                analyzed = analyze_stream(
                    stream_url=stream_url,
                    stream_id=stream_id,
                    stream_name=stream_name,
                    ffmpeg_duration=analysis_params.get('ffmpeg_duration', 20),
                    timeout=analysis_params.get('timeout', 30),
                    retries=analysis_params.get('retries', 1),
                    retry_delay=analysis_params.get('retry_delay', 10),
                    user_agent=analysis_params.get('user_agent', 'VLC/3.0.14'),
                    stream_startup_buffer=analysis_params.get('stream_startup_buffer', 10),
                    proxy=proxy
                )
                
                # Check if analysis was successful (not dead/error)
                if not self._is_stream_dead(analyzed) and analyzed.get('status') == 'OK':
                    logger.info(f"Stream {stream_id}: ✅ SUCCESS with available profile {profile_name} (ID: {profile_id})")
                    # Add profile info to result
                    analyzed['used_profile_id'] = profile_id
                    analyzed['used_profile_name'] = profile_name
                    analyzed['profile_failover_attempts'] = profile_idx
                    analyzed['profile_failover_phase'] = 1
                    return analyzed
                else:
                    # Profile failed, try next one
                    status = analyzed.get('status', 'Unknown')
                    logger.warning(f"Stream {stream_id}: ❌ FAILED with available profile {profile_name} (ID: {profile_id}) - Status: {status}")
                    last_error = analyzed
                    
            except Exception as e:
                logger.error(f"Stream {stream_id}: ❌ ERROR with available profile {profile_name} (ID: {profile_id}): {e}")
                last_error = {
                    'stream_id': stream_id,
                    'stream_name': stream_name,
                    'stream_url': stream.get('url', ''),
                    'status': 'Error',
                    'error': str(e)
                }
        
        # Phase 2: All available profiles failed - try ALL profiles (including full ones) with intelligent polling
        if udi and stream.get('m3u_account'):
            all_profiles = udi.get_all_profiles_for_stream(stream)
            # Filter out profiles we already tried
            tried_profile_ids = {p.get('id') for p in available_profiles}
            remaining_profiles = [p for p in all_profiles if p.get('id') not in tried_profile_ids]
            
            if remaining_profiles:
                # Get Phase 2 configuration
                phase2_enabled = self.config.get('profile_failover', {}).get('try_full_profiles', True)
                
                if not phase2_enabled:
                    logger.info(f"Stream {stream_id} ({stream_name}): Phase 2 disabled, skipping {len(remaining_profiles)} full profile(s)")
                else:
                    phase2_max_wait = self.config.get('profile_failover', {}).get('phase2_max_wait', 600)
                    phase2_poll_interval = self.config.get('profile_failover', {}).get('phase2_poll_interval', 10)
                    
                    logger.warning(f"Stream {stream_id} ({stream_name}): Phase 2 - All available profiles failed, trying {len(remaining_profiles)} additional profile(s) with intelligent polling")
                    logger.info(f"Stream {stream_id}: Phase 2 config - max_wait: {phase2_max_wait}s, poll_interval: {phase2_poll_interval}s")
                    
                    import time
                    start_time = time.time()
                    tested_profile_ids = set(tried_profile_ids)  # Track all tested profiles
                    
                    # Intelligent polling loop
                    while remaining_profiles and (time.time() - start_time) < phase2_max_wait:
                        # Check which profiles are NOW available (have free slots)
                        currently_available = udi.get_all_available_profiles_for_stream(stream)
                        currently_available_ids = {p.get('id') for p in currently_available}
                        
                        # Find profiles that are NOW available AND not yet tested
                        newly_available = [
                            p for p in remaining_profiles 
                            if p.get('id') in currently_available_ids 
                            and p.get('id') not in tested_profile_ids
                        ]
                        
                        if newly_available:
                            # Test the first newly available profile
                            profile = newly_available[0]
                            profile_id = profile.get('id')
                            profile_name = profile.get('name', f'Profile {profile_id}')
                            
                            try:
                                # Apply URL transformation for this specific profile
                                stream_url = udi.apply_profile_url_transformation(stream, profile)
                                
                                elapsed = time.time() - start_time
                                logger.info(f"Stream {stream_id}: Testing newly available profile {profile_name} (ID: {profile_id}) [elapsed: {elapsed:.1f}s]")
                                
                                proxy = get_stream_proxy(stream_id)
                                
                                analyzed = analyze_stream(
                                    stream_url=stream_url,
                                    stream_id=stream_id,
                                    stream_name=stream_name,
                                    ffmpeg_duration=analysis_params.get('ffmpeg_duration', 20),
                                    timeout=analysis_params.get('timeout', 30),
                                    retries=analysis_params.get('retries', 1),
                                    retry_delay=analysis_params.get('retry_delay', 10),
                                    user_agent=analysis_params.get('user_agent', 'VLC/3.0.14'),
                                    stream_startup_buffer=analysis_params.get('stream_startup_buffer', 10),
                                    proxy=proxy
                                )
                                
                                # Mark profile as tested
                                tested_profile_ids.add(profile_id)
                                remaining_profiles = [p for p in remaining_profiles if p.get('id') != profile_id]
                                
                                # Check if analysis was successful
                                if not self._is_stream_dead(analyzed) and analyzed.get('status') == 'OK':
                                    logger.info(f"Stream {stream_id}: ✅ SUCCESS with profile {profile_name} (ID: {profile_id}) in Phase 2")
                                    # Add profile info to result
                                    analyzed['used_profile_id'] = profile_id
                                    analyzed['used_profile_name'] = profile_name
                                    analyzed['profile_failover_attempts'] = len(tested_profile_ids)
                                    analyzed['profile_failover_phase'] = 2
                                    analyzed['phase2_elapsed_time'] = time.time() - start_time
                                    return analyzed
                                else:
                                    # Profile failed
                                    status = analyzed.get('status', 'Unknown')
                                    logger.warning(f"Stream {stream_id}: ❌ FAILED with profile {profile_name} (ID: {profile_id}) - Status: {status}")
                                    last_error = analyzed
                                    
                            except Exception as e:
                                logger.error(f"Stream {stream_id}: ❌ ERROR with profile {profile_name} (ID: {profile_id}): {e}")
                                tested_profile_ids.add(profile_id)
                                remaining_profiles = [p for p in remaining_profiles if p.get('id') != profile_id]
                                last_error = {
                                    'stream_id': stream_id,
                                    'stream_name': stream_name,
                                    'stream_url': stream.get('url', ''),
                                    'status': 'Error',
                                    'error': str(e)
                                }
                        else:
                            # No profiles available right now, wait and check again
                            if remaining_profiles:
                                elapsed = time.time() - start_time
                                remaining_time = phase2_max_wait - elapsed
                                logger.debug(f"Stream {stream_id}: No profiles available, waiting {phase2_poll_interval}s (elapsed: {elapsed:.1f}s, remaining: {remaining_time:.1f}s)")
                                time.sleep(phase2_poll_interval)
                    
                    # Phase 2 completed or timed out
                    elapsed = time.time() - start_time
                    if remaining_profiles:
                        logger.warning(f"Stream {stream_id}: Phase 2 timeout after {elapsed:.1f}s, {len(remaining_profiles)} profile(s) not tested")
                    else:
                        logger.info(f"Stream {stream_id}: Phase 2 completed after {elapsed:.1f}s, all profiles tested")
        
        # All profiles failed (both phases)
        total_attempts = len(available_profiles) + len(remaining_profiles) if 'remaining_profiles' in locals() else len(available_profiles)
        logger.error(f"Stream {stream_id} ({stream_name}): ❌ ALL {total_attempts} profile(s) FAILED (Phase 1 + Phase 2) - marking as dead")
        
        # Return the last error result
        if last_error:
            last_error['profile_failover_attempts'] = total_attempts
            last_error['all_profiles_failed'] = True
            return last_error
        
        # Fallback error result
        return {
            'stream_id': stream_id,
            'stream_name': stream_name,
            'stream_url': stream.get('url', ''),
            'timestamp': datetime.now().isoformat(),
            'video_codec': 'N/A',
            'audio_codec': 'N/A',
            'resolution': '0x0',
            'fps': 0,
            'bitrate_kbps': None,
            'status': 'Error',
            'profile_failover_attempts': total_attempts if 'total_attempts' in locals() else 0,
            'all_profiles_failed': True
        }
    
    def _calculate_stream_score(self, stream_data: Dict, channel_id: Optional[int] = None) -> float:
        """Calculate a quality score for a stream based on analysis.
        
        Applies M3U account priority bonuses according to priority_mode and
        channel-specific quality preferences.
        
        Args:
            stream_data: Stream analysis data
            channel_id: Optional channel ID for applying quality preferences
        """
        # Dead streams always get a score of 0
        if self._is_stream_dead(stream_data):
            return 0.0
        
        # Fallback scoring for streams without bitrate but with resolution/FPS
        # These streams work but FFmpeg couldn't extract all metadata (e.g., due to packet corruption)
        if (stream_data.get('bitrate_kbps', 0) == 0 and 
            stream_data.get('resolution') not in ['0x0', 'N/A', ''] and
            stream_data.get('fps', 0) > 0):
            
            logger.debug(f"Stream without bitrate info but functional: {stream_data.get('resolution')}@{stream_data.get('fps')}fps - assigning fallback score: 0.40")
            return 0.40  # Medium score - better than dead streams (0.0), worse than complete streams (0.60-1.0)
        
        weights = self.config.get('scoring.weights', {})
        score = 0.0
        
        # Bitrate score (0-1, normalized to typical range 1000-8000 kbps)
        bitrate = stream_data.get('bitrate_kbps', 0)
        if isinstance(bitrate, (int, float)) and bitrate > 0:
            bitrate_score = min(bitrate / 8000, 1.0)
            score += bitrate_score * weights.get('bitrate', 0.40)
        
        # Resolution score (0-1)
        resolution = stream_data.get('resolution', 'N/A')
        resolution_score = 0.0
        if 'x' in str(resolution):
            try:
                width, height = map(int, resolution.split('x'))
                # Score based on vertical resolution
                if height >= 1080:
                    resolution_score = 1.0
                elif height >= 720:
                    resolution_score = 0.7
                elif height >= 576:
                    resolution_score = 0.5
                else:
                    resolution_score = 0.3
            except (ValueError, AttributeError):
                pass
        score += resolution_score * weights.get('resolution', 0.35)
        
        # FPS score (0-1)
        fps = stream_data.get('fps', 0)
        if isinstance(fps, (int, float)) and fps > 0:
            fps_score = min(fps / 60, 1.0)
            score += fps_score * weights.get('fps', 0.15)
        
        # Codec score (0-1)
        codec = stream_data.get('video_codec', '').lower()
        codec_score = 0.0
        if codec:
            if 'h265' in codec or 'hevc' in codec:
                codec_score = 1.0 if self.config.get('scoring.prefer_h265', True) else 0.8
            elif 'h264' in codec or 'avc' in codec:
                codec_score = 0.8 if self.config.get('scoring.prefer_h265', True) else 1.0
            elif codec != 'n/a':
                codec_score = 0.5
        score += codec_score * weights.get('codec', 0.10)
        
        # Apply M3U account priority bonus if enabled
        stream_id = stream_data.get('stream_id')
        if stream_id:
            priority_boost = self._get_priority_boost(stream_id, stream_data)
            score += priority_boost
        
        # Apply channel-specific quality preference boost/penalty
        if channel_id:
            quality_boost = self._get_quality_preference_boost(stream_data, channel_id)
            score += quality_boost
        
        return round(score, 2)
    
    def _get_priority_boost(self, stream_id: int, stream_data: Dict) -> float:
        """Calculate priority boost for a stream based on its M3U account priority.
        
        Args:
            stream_id: The stream ID
            stream_data: Stream data dictionary containing resolution and other info
            
        Returns:
            Priority boost value (0.0 to 10.0+)
        """
        try:
            # Get stream from UDI to find its M3U account
            udi = get_udi_manager()
            stream = udi.get_stream_by_id(stream_id)
            if not stream:
                return 0.0
            
            m3u_account_id = stream.get('m3u_account')
            if not m3u_account_id:
                return 0.0
            
            # Get M3U account to check priority settings
            m3u_account = udi.get_m3u_account_by_id(m3u_account_id)
            if not m3u_account:
                return 0.0
            
            priority = m3u_account.get('priority', 0)
            priority_mode = m3u_account.get('priority_mode', 'disabled')
            
            # If priority is 0 or mode is disabled, no boost
            if priority == 0 or priority_mode == 'disabled':
                return 0.0
            
            # For "all_streams" mode, apply full priority boost to all streams
            if priority_mode == 'all_streams':
                # Priority boost: each priority point adds 0.5 to the score
                # This ensures higher priority accounts' streams rank higher
                boost = priority * 0.5
                logger.debug(f"Applying all_streams priority boost of {boost} to stream {stream_id} (priority: {priority})")
                return boost
            
            # For "same_resolution" mode, we need to group streams by resolution
            # and only apply priority within the same resolution group
            # This is more complex and requires comparing with other streams
            # For now, we apply a smaller boost that respects quality differences
            elif priority_mode == 'same_resolution':
                # Apply a smaller boost that won't override resolution differences
                # but will prioritize within same resolution
                boost = priority * 0.2
                logger.debug(f"Applying same_resolution priority boost of {boost} to stream {stream_id} (priority: {priority})")
                return boost
            
            return 0.0
        except Exception as e:
            logger.error(f"Error calculating priority boost for stream {stream_id}: {e}")
            return 0.0
    
    def _get_quality_preference_boost(self, stream_data: Dict, channel_id: int) -> float:
        """Calculate quality preference boost/penalty for a stream based on channel settings.
        
        Args:
            stream_data: Stream data dictionary containing resolution and other info
            channel_id: The channel ID to get quality preferences for
            
        Returns:
            Quality preference boost/penalty value (-10.0 to 0.5)
        """
        try:
            from channel_settings_manager import get_channel_settings_manager
            
            settings_manager = get_channel_settings_manager()
            
            # Get channel's group ID for inheritance
            udi = get_udi_manager()
            channel = udi.get_channel_by_id(channel_id)
            channel_group_id = channel.get('channel_group_id') if channel else None
            
            # Get effective settings with inheritance
            effective_settings = settings_manager.get_channel_effective_settings(channel_id, channel_group_id)
            quality_pref = effective_settings.get('quality_preference', 'default')
            
            if quality_pref == 'default':
                return 0.0
            
            resolution = stream_data.get('resolution', 'N/A')
            if 'x' in str(resolution):
                try:
                    width, height = map(int, resolution.split('x'))
                    
                    if quality_pref == 'prefer_4k' and height >= 2160:
                        # Extra bonus for 4K streams
                        logger.debug(f"Applying 4K preference bonus (+0.5) to stream for channel {channel_id}")
                        return 0.5
                    elif quality_pref == 'avoid_4k' and height >= 2160:
                        # Penalty for 4K streams (prefer Full HD instead)
                        logger.debug(f"Applying 4K avoidance penalty (-0.5) to stream for channel {channel_id}")
                        return -0.5
                    elif quality_pref == 'max_1080p' and height > 1080:
                        # Heavy penalty for streams above 1080p (effectively exclude them)
                        logger.debug(f"Applying max 1080p penalty (-10.0) to stream for channel {channel_id}")
                        return -10.0
                    elif quality_pref == 'max_720p' and height > 720:
                        # Heavy penalty for streams above 720p (effectively exclude them)
                        logger.debug(f"Applying max 720p penalty (-10.0) to stream for channel {channel_id}")
                        return -10.0
                        
                except (ValueError, AttributeError):
                    pass
            
            return 0.0
        except Exception as e:
            logger.error(f"Error calculating quality preference boost for channel {channel_id}: {e}")
            return 0.0
    
    def get_status(self) -> Dict:
        """Get current service status."""
        queue_status = self.check_queue.get_status()
        progress = self.progress.get()
        
        # Stream checking mode is active when:
        # - A global action is in progress, OR
        # - An individual channel is being checked, OR
        # - There are channels in the queue waiting to be checked
        stream_checking_mode = (
            self.global_action_in_progress or 
            self.checking or 
            queue_status.get('queue_size', 0) > 0 or
            queue_status.get('in_progress', 0) > 0
        )
        
        return {
            'running': self.running,
            'checking': self.checking,
            'global_action_in_progress': self.global_action_in_progress,
            'stream_checking_mode': stream_checking_mode,
            'enabled': self.config.get('enabled', True),
            'queue': queue_status,
            'progress': progress,
            'last_global_check': self.update_tracker.get_last_global_check(),
            'config': {
                'automation_controls': self.config.get('automation_controls', {}),
                'check_interval': self.config.get('check_interval'),
                'global_check_schedule': self.config.get('global_check_schedule'),
                'queue_settings': self.config.get('queue')
            }
        }
    
    def queue_channel(self, channel_id: int, priority: int = 10, force_check: bool = False) -> bool:
        """Manually queue a channel for checking.
        
        Args:
            channel_id: ID of the channel to queue
            priority: Priority for queue ordering (higher = earlier)
            force_check: If True, marks channel for force checking (bypasses 2-hour immunity)
            
        Returns:
            True if channel was successfully queued, False otherwise
        """
        if force_check:
            self.update_tracker.mark_channel_for_force_check(channel_id)
            logger.info(f"Marked channel {channel_id} for force check (bypasses 2-hour immunity)")
        return self.check_queue.add_channel(channel_id, priority)
    
    def queue_channels(self, channel_ids: List[int], priority: int = 10, force_check: bool = False) -> int:
        """Manually queue multiple channels for checking.
        
        Args:
            channel_ids: List of channel IDs to queue
            priority: Priority for queue ordering (higher = earlier)
            force_check: If True, marks all channels for force checking (bypasses 2-hour immunity)
            
        Returns:
            Number of channels successfully queued
        """
        if force_check:
            for channel_id in channel_ids:
                self.update_tracker.mark_channel_for_force_check(channel_id)
            logger.info(f"Marked {len(channel_ids)} channels for force check (bypasses 2-hour immunity)")
        return self.check_queue.add_channels(channel_ids, priority)
    
    def check_single_channel(self, channel_id: int, program_name: Optional[str] = None) -> Dict:
        """Check a single channel immediately and return results.
        
        This performs a targeted channel refresh for a single channel:
        - Identifies M3U accounts used by the channel
        - Refreshes playlists for accounts associated with the channel
        - Clears dead streams for the specified channel to give them a second chance (like global action)
        - Re-matches and assigns streams (including previously dead ones) if matching_mode is enabled
        - Force checks all streams (bypasses 2-hour immunity) if checking_mode is enabled
        - Detects newly dead streams and marks them (if checking is enabled)
        - Detects revived streams and marks them as alive (if checking is enabled)
        - Removes dead streams from the channel (if checking is enabled)
        
        Note: This now works like Global Action but only for the specified channel.
        Dead streams for other channels are not affected.
        
        Channel settings (matching_mode and checking_mode) are respected:
        - If matching_mode is disabled, stream matching is skipped
        - If checking_mode is disabled, stream quality checking is skipped
        
        Args:
            channel_id: ID of the channel to check
            program_name: Optional program name if this is a scheduled EPG check
            
        Returns:
            Dict with check results and statistics
        """
        import time as time_module
        start_time = time_module.time()
        
        try:
            logger.info(f"Starting single channel check for channel {channel_id}")
            
            # Get channel info from UDI
            udi = get_udi_manager()
            channel = udi.get_channel_by_id(channel_id)
            if not channel:
                error_msg = f"Channel {channel_id} not found"
                logger.error(error_msg)
                return {'success': False, 'error': error_msg}
            
            channel_name = channel.get('name', f'Channel {channel_id}')
            
            # Check channel settings for matching and checking modes
            channel_settings = get_channel_settings_manager()
            settings = channel_settings.get_channel_settings(channel_id)
            matching_enabled = settings['matching_mode'] == 'enabled'
            checking_enabled = settings['checking_mode'] == 'enabled'
            
            logger.info(f"Channel {channel_name} settings: matching={matching_enabled}, checking={checking_enabled}")
            
            # Check if channel has active viewers or if its playlist has reached max concurrent streams
            current_streams = fetch_channel_streams(channel_id)
            if current_streams:
                limit_check_result = self._check_channel_limits(channel_id, channel_name, current_streams)
                if limit_check_result is not None:
                    # Convert the internal result format to the single channel check format
                    return {
                        'success': False,
                        'error': f"Channel check skipped: {limit_check_result.get('skip_reason', 'limits reached')}",
                        'reason': limit_check_result.get('skip_reason'),
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'details': limit_check_result
                    }
            
            # Step 1: Identify M3U accounts for channel (reusing current_streams from limit check above)
            logger.info(f"Step 1/6: Identifying M3U accounts for channel {channel_name}...")
            account_ids = set()
            if current_streams:
                for stream in current_streams:
                    m3u_account = stream.get('m3u_account')
                    if m3u_account:
                        account_ids.add(m3u_account)
            
            # Also check dead streams for this channel to find M3U accounts
            # This fixes the bug where channels with all dead streams couldn't refresh their playlists
            dead_streams = self.dead_streams_tracker.get_dead_streams_for_channel(channel_id)
            for dead_url, dead_info in dead_streams.items():
                # Try to get the stream from UDI to find its m3u_account
                stream_id = dead_info.get('stream_id')
                if stream_id:
                    stream = udi.get_stream_by_id(stream_id)
                    if stream:
                        m3u_account = stream.get('m3u_account')
                        if m3u_account:
                            account_ids.add(m3u_account)
                            logger.info(f"Found M3U account {m3u_account} from dead stream {dead_info.get('stream_name', 'Unknown')}")
            
            # Step 2: Refresh playlists for those accounts
            if account_ids:
                logger.info(f"Step 2/6: Refreshing playlists for {len(account_ids)} M3U account(s)...")
                # Import here to allow better test mocking
                from api_utils import refresh_m3u_playlists
                for account_id in account_ids:
                    logger.info(f"Refreshing M3U account {account_id}")
                    refresh_m3u_playlists(account_id=account_id)
                
                # Refresh UDI cache to get updated streams
                # Also refresh M3U accounts to detect any new accounts
                # And refresh channel groups to detect any group changes
                udi.refresh_m3u_accounts()  # Check for new M3U accounts
                udi.refresh_streams()
                udi.refresh_channels()
                udi.refresh_channel_groups()  # Check for new/updated channel groups
                logger.info("✓ Playlists refreshed and UDI cache updated")
            else:
                logger.info("Step 2/6: No M3U accounts found for this channel, skipping playlist refresh")
            
            # Step 3: Clear dead streams for this channel to give them a second chance
            logger.info(f"Step 3/6: Clearing dead streams for channel {channel_name} to give them a second chance...")
            try:
                # Clear all dead streams that belong to this channel by channel_id
                # This handles cases where playlist refresh creates new streams with different URLs
                cleared_count = self.dead_streams_tracker.remove_dead_streams_by_channel_id(channel_id)
                
                if cleared_count > 0:
                    logger.info(f"✓ Cleared {cleared_count} dead stream(s) from tracker - they will be given a second chance")
                else:
                    logger.info("✓ No dead streams to clear for this channel")
            except Exception as e:
                logger.error(f"✗ Failed to clear dead streams: {e}")
            
            # Step 4: Validate existing streams against regex patterns (if matching is enabled)
            if matching_enabled:
                logger.info(f"Step 4/6: Validating existing streams for channel {channel_name}...")
                try:
                    from automated_stream_manager import AutomatedStreamManager
                    automation_manager = AutomatedStreamManager()
                    
                    # Run validation - respects automation_controls.remove_non_matching_streams setting
                    validation_results = automation_manager.validate_and_remove_non_matching_streams()
                    if validation_results.get("streams_removed", 0) > 0:
                        logger.info(f"✓ Removed {validation_results['streams_removed']} non-matching streams")
                    else:
                        logger.info("✓ No non-matching streams found to remove")
                except Exception as e:
                    logger.error(f"✗ Failed to validate streams: {e}")
            else:
                logger.info(f"Step 4/6: Skipping stream validation (matching is disabled for this channel)")
            
            # Step 5: Re-match and assign streams for this specific channel (if matching is enabled)
            # With dead streams cleared, previously dead streams can now be re-added
            if matching_enabled:
                logger.info(f"Step 5/6: Re-matching streams for channel {channel_name}...")
                try:
                    # Import here to allow better test mocking
                    from automated_stream_manager import AutomatedStreamManager
                    automation_manager = AutomatedStreamManager()
                    
                    # Run full discovery (this will add new matching streams but skip dead ones)
                    # Skip automatic check trigger since we'll perform the check explicitly in Step 6
                    assignments = automation_manager.discover_and_assign_streams(force=True, skip_check_trigger=True)
                    if assignments:
                        logger.info(f"✓ Stream matching completed")
                    else:
                        logger.info("✓ No new stream assignments")
                except Exception as e:
                    logger.error(f"✗ Failed to match streams: {e}")
            else:
                logger.info(f"Step 5/6: Skipping stream matching (matching is disabled for this channel)")
            
            # Step 6: Mark channel for force check and perform the check (if checking is enabled)
            dead_count = 0
            if checking_enabled:
                logger.info(f"Step 6/6: Force checking all streams for channel {channel_name}...")
                self.update_tracker.mark_channel_for_force_check(channel_id)
                
                # Perform the check (this will now bypass immunity and check all streams)
                # Returns dict with dead_streams_count and revived_streams_count
                # Skip batch changelog since this is a single channel check
                check_result = self._check_channel(channel_id, skip_batch_changelog=True)
                if not check_result or not isinstance(check_result, dict):
                    # This should not happen with updated methods, but provide safe fallback
                    logger.warning(f"_check_channel did not return expected result dict, using defaults")
                    check_result = {'dead_streams_count': 0, 'revived_streams_count': 0}
                
                # Get the count of dead streams that were removed during the check
                dead_count = check_result.get('dead_streams_count', 0)
            else:
                logger.info(f"Step 6/6: Skipping stream checking (checking is disabled for this channel)")
            
            # Gather statistics after check using centralized utility
            streams = fetch_channel_streams(channel_id)
            total_streams = len(streams)
            
            # Calculate channel averages using centralized function
            channel_averages = calculate_channel_averages(streams, dead_stream_ids=set())
            
            check_stats = {
                'total_streams': total_streams,
                'dead_streams': dead_count,
                'avg_resolution': channel_averages['avg_resolution'],
                'avg_bitrate': channel_averages['avg_bitrate'],
                'avg_fps': channel_averages['avg_fps'],
                'stream_details': []
            }
            
            # Add top stream details using centralized extraction
            for stream in streams[:10]:  # Top 10 streams
                # Extract stats using centralized utility
                extracted_stats = extract_stream_stats(stream)
                formatted_stats = format_stream_stats_for_display(extracted_stats)
                
                # Calculate score for this stream using its stats
                # The score needs to be calculated from the stream_stats data stored in Dispatcharr
                stream_stats = stream.get('stream_stats', {})
                if stream_stats is None:
                    stream_stats = {}
                if isinstance(stream_stats, str):
                    try:
                        stream_stats = json.loads(stream_stats)
                        if stream_stats is None:
                            stream_stats = {}
                    except json.JSONDecodeError:
                        stream_stats = {}
                
                # Build stream data dict for score calculation
                score_data = {
                    'stream_id': stream.get('id'),
                    'stream_name': stream.get('name', 'Unknown'),
                    'stream_url': stream.get('url', ''),
                    'resolution': stream_stats.get('resolution', '0x0'),
                    'fps': stream_stats.get('source_fps', 0),
                    'video_codec': stream_stats.get('video_codec', 'N/A'),
                    'bitrate_kbps': stream_stats.get('ffmpeg_output_bitrate', 0)
                }
                
                # Calculate score
                score = self._calculate_stream_score(score_data)
                
                # Get M3U account name for this stream using helper method
                m3u_account_name = None
                m3u_account_id = stream.get('m3u_account')
                if m3u_account_id:
                    m3u_account_name = self._get_m3u_account_name(stream.get('id'), udi)
                
                check_stats['stream_details'].append({
                    'stream_id': stream.get('id'),
                    'stream_name': stream.get('name', 'Unknown'),
                    'resolution': formatted_stats['resolution'],
                    'bitrate': formatted_stats['bitrate'],
                    'video_codec': formatted_stats['video_codec'],
                    'fps': formatted_stats['fps'],
                    'score': score,
                    'm3u_account': m3u_account_name
                })
            
            # Calculate duration
            end_time = time_module.time()
            duration_seconds = int(end_time - start_time)
            
            # Format duration as human-readable string
            if duration_seconds < 60:
                duration_str = f"{duration_seconds}s"
            elif duration_seconds < 3600:
                minutes = duration_seconds // 60
                seconds = duration_seconds % 60
                duration_str = f"{minutes}m {seconds}s"
            else:
                hours = duration_seconds // 3600
                minutes = (duration_seconds % 3600) // 60
                duration_str = f"{hours}h {minutes}m"
            
            # Add duration to check stats
            check_stats['duration'] = duration_str
            check_stats['duration_seconds'] = duration_seconds
            
            # Add changelog entry
            if self.changelog:
                try:
                    # Get logo URL for the channel
                    logo_url = None
                    logo_id = channel.get('logo_id')
                    if logo_id:
                        try:
                            logo = udi.get_logo_by_id(logo_id)
                            if logo and logo.get('cache_url'):
                                logo_url = logo.get('cache_url')
                        except Exception as e:
                            logger.debug(f"Could not fetch logo for channel {channel_id}: {e}")
                    
                    self.changelog.add_single_channel_check_entry(
                        channel_id=channel_id,
                        channel_name=channel_name,
                        check_stats=check_stats,
                        logo_url=logo_url,
                        program_name=program_name
                    )
                except Exception as e:
                    logger.warning(f"Failed to add changelog entry: {e}")
            
            logger.info(f"✓ Single channel check completed for {channel_name} in {duration_str}")
            
            # Trigger empty channel disabling if configured
            # This ensures that if this channel became empty after checking, it gets disabled
            self._trigger_empty_channel_disabling()
            
            return {
                'success': True,
                'channel_id': channel_id,
                'channel_name': channel_name,
                'stats': check_stats
            }
            
        except Exception as e:
            logger.error(f"Error checking single channel {channel_id}: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
    
    def clear_queue(self):
        """Clear the checking queue."""
        self.check_queue.clear()
        logger.info("Checking queue cleared")
    
    def trigger_check_updated_channels(self):
        """Trigger immediate check of channels with M3U updates.
        
        This method signals the scheduler to immediately process any channels
        that have been marked as updated, instead of waiting for the next
        scheduled check interval.
        """
        if self.running:
            logger.info("Triggering immediate check for updated channels")
            self.check_trigger.set()
        else:
            logger.warning("Cannot trigger check - service is not running")
    
    def update_config(self, updates: Dict):
        """Update service configuration and apply changes immediately."""
        # Sanitize user_agent if present
        if 'stream_analysis' in updates and 'user_agent' in updates['stream_analysis']:
            user_agent = updates['stream_analysis']['user_agent']
            # Sanitize user agent: allow alphanumeric, spaces, dots, slashes, dashes, underscores, parentheses
            import re
            sanitized = re.sub(r'[^a-zA-Z0-9 ./_\-()]+', '', str(user_agent))
            # Limit length to 200 characters
            sanitized = sanitized[:200].strip()
            if not sanitized:
                sanitized = 'VLC/3.0.14'  # Default fallback
            updates['stream_analysis']['user_agent'] = sanitized
            if sanitized != user_agent:
                logger.warning(f"User agent sanitized from '{user_agent}' to '{sanitized}'")
        
        # Log what's being updated
        config_changes = []
        if 'automation_controls' in updates:
            old_controls = self.config.get('automation_controls', {})
            new_controls = updates['automation_controls']
            for key, value in new_controls.items():
                old_value = old_controls.get(key, False)
                if old_value != value:
                    config_changes.append(f"Automation control '{key}': {old_value} → {value}")
        
        if 'account_stream_limits' in updates:
            limits_changes = []
            limits = updates['account_stream_limits']
            if 'enabled' in limits:
                old_enabled = self.config.get('account_stream_limits', {}).get('enabled', True)
                new_enabled = limits['enabled']
                if old_enabled != new_enabled:
                    limits_changes.append(f"Enabled: {old_enabled} → {new_enabled}")
            if 'global_limit' in limits:
                old_global = self.config.get('account_stream_limits', {}).get('global_limit', 0)
                new_global = limits['global_limit']
                if old_global != new_global:
                    limits_changes.append(f"Global limit: {old_global} → {new_global}")
            if 'account_limits' in limits:
                old_account_limits = self.config.get('account_stream_limits', {}).get('account_limits', {})
                new_account_limits = limits['account_limits']
                if old_account_limits != new_account_limits:
                    old_count = len(old_account_limits)
                    new_count = len(new_account_limits)
                    limits_changes.append(f"Account-specific limits: {old_count} → {new_count} accounts")
            if limits_changes:
                config_changes.append(f"Account stream limits: {', '.join(limits_changes)}")
        
        if 'global_check_schedule' in updates:
            schedule_changes = []
            schedule = updates['global_check_schedule']
            if 'hour' in schedule or 'minute' in schedule:
                old_hour = self.config.get('global_check_schedule.hour', 3)
                old_minute = self.config.get('global_check_schedule.minute', 0)
                new_hour = schedule.get('hour', old_hour)
                new_minute = schedule.get('minute', old_minute)
                if old_hour != new_hour or old_minute != new_minute:
                    schedule_changes.append(f"Time: {old_hour:02d}:{old_minute:02d} → {new_hour:02d}:{new_minute:02d}")
            if 'frequency' in schedule:
                old_freq = self.config.get('global_check_schedule.frequency', 'daily')
                new_freq = schedule['frequency']
                if old_freq != new_freq:
                    schedule_changes.append(f"Frequency: {old_freq} → {new_freq}")
            if 'enabled' in schedule:
                old_enabled = self.config.get('global_check_schedule.enabled', True)
                new_enabled = schedule['enabled']
                if old_enabled != new_enabled:
                    schedule_changes.append(f"Enabled: {old_enabled} → {new_enabled}")
            if schedule_changes:
                config_changes.append(f"Global check schedule: {', '.join(schedule_changes)}")
        
        # Apply the configuration update
        # For account_stream_limits, we need to completely replace the section
        # to handle deleted account limits properly
        if 'account_stream_limits' in updates:
            # Directly access the config dictionary and replace the entire section
            self.config.config['account_stream_limits'] = updates['account_stream_limits']
            # Remove account_stream_limits from updates to avoid double-processing
            updates_copy = updates.copy()
            del updates_copy['account_stream_limits']
            if updates_copy:  # Only update if there are other changes
                self.config.update(updates_copy)
            # Save the config manually since we bypassed the update method
            self.config._save_config()
        else:
            # Use the normal update method for other changes
            self.config.update(updates)
        
        # Save configuration to file (only if we didn't already save it above)
        if 'account_stream_limits' not in updates:
            self._save_config()
        
        # Log the changes
        if config_changes:
            logger.info(f"Configuration updated: {'; '.join(config_changes)}")
        else:
            logger.info("Configuration updated")
        
        # Signal that config has changed for immediate application
        if self.running:
            self.config_changed.set()
            # Wake up the scheduler immediately by setting the trigger
            # The scheduler will check config_changed and skip channel queueing
            self.check_trigger.set()
            logger.info("Configuration changes will be applied immediately")
        
        # Reload queue max size if changed
        if 'queue' in updates and 'max_size' in updates['queue']:
            # Can't resize existing queue, but will apply on next restart
            logger.info("Queue max size updated, will apply on next restart")
    
    def trigger_global_action(self):
        """Manually trigger a global action (Update, Match, Check all channels).
        
        This can be called at any time to perform a complete global action,
        regardless of the scheduled time.
        """
        if not self.running:
            logger.warning("Cannot trigger global action - service is not running")
            return False
        
        logger.info("Manual global action triggered")
        try:
            self._perform_global_action()
            self.update_tracker.mark_global_check()
            return True
        except Exception as e:
            logger.error(f"Failed to trigger global action: {e}")
            return False
    
    def rescore_and_resort_all_channels(self):
        """Re-calculate scores and re-sort all channels using existing stream stats.
        
        This function does NOT perform new quality checks. Instead, it:
        1. Gets all channels and their streams
        2. Uses existing stream_stats from UDI cache
        3. Re-calculates scores based on current config (priorities, quality preferences, weights)
        4. Re-sorts streams by score
        5. Re-applies account stream limits
        6. Updates channel-stream assignments
        
        Useful after changing:
        - M3U account priorities
        - Account stream limits
        - Quality preferences
        - Scoring weights
        - Provider diversification settings
        
        Returns:
            Dict with statistics about the re-scoring operation
        """
        if not self.running:
            logger.warning("Cannot re-score channels - service is not running")
            return {'success': False, 'error': 'Service is not running'}
        
        logger.info("=" * 80)
        logger.info("RE-SCORE & RE-SORT ALL CHANNELS (using existing stats)")
        logger.info("=" * 80)
        
        start_time = time_module.time()
        
        try:
            udi = get_udi_manager()
            channels = udi.get_channels()
            
            if not channels:
                logger.warning("No channels found")
                return {'success': False, 'error': 'No channels found'}
            
            stats = {
                'channels_processed': 0,
                'channels_updated': 0,
                'total_streams_before': 0,
                'total_streams_after': 0,
                'streams_removed_by_limits': 0,
                'channels_with_changes': []
            }
            
            account_limits_config = self.config.get('account_stream_limits', {})
            
            for channel in channels:
                channel_id = channel.get('id')
                channel_name = channel.get('name', f'Channel {channel_id}')
                
                if not channel_id:
                    continue
                
                logger.info(f"Processing channel: {channel_name} (ID: {channel_id})")
                
                # Get current streams for this channel
                current_streams = udi.get_channel_streams(channel_id)
                if not current_streams:
                    logger.debug(f"Channel {channel_name} has no streams, skipping")
                    continue
                
                stats['channels_processed'] += 1
                streams_before = len(current_streams)
                stats['total_streams_before'] += streams_before
                
                # Build analyzed stream data from existing stats
                analyzed_streams = []
                
                for stream in current_streams:
                    stream_id = stream.get('id')
                    if not stream_id:
                        continue
                    
                    # Get full stream data with stats
                    full_stream = udi.get_stream_by_id(stream_id)
                    if not full_stream:
                        continue
                    
                    # Get stream_stats
                    stream_stats = full_stream.get('stream_stats')
                    if stream_stats is None:
                        stream_stats = {}
                    if isinstance(stream_stats, str):
                        try:
                            stream_stats = json.loads(stream_stats)
                            if stream_stats is None:
                                stream_stats = {}
                        except json.JSONDecodeError:
                            stream_stats = {}
                    
                    # Build stream data dict for score calculation
                    score_data = {
                        'stream_id': stream_id,
                        'stream_name': stream.get('name', 'Unknown'),
                        'stream_url': stream.get('url', ''),
                        'resolution': stream_stats.get('resolution', '0x0'),
                        'fps': stream_stats.get('source_fps', 0),
                        'video_codec': stream_stats.get('video_codec', 'N/A'),
                        'audio_codec': stream_stats.get('audio_codec', 'N/A'),
                        'bitrate_kbps': stream_stats.get('ffmpeg_output_bitrate', 0),
                        'status': 'OK' if stream_stats.get('resolution') not in ['0x0', 'N/A', ''] else 'Error'
                    }
                    
                    # Calculate score with current config
                    score = self._calculate_stream_score(score_data, channel_id)
                    score_data['score'] = score
                    
                    analyzed_streams.append(score_data)
                
                if not analyzed_streams:
                    logger.debug(f"No streams with stats found for channel {channel_name}")
                    continue
                
                # Sort by score (highest first)
                analyzed_streams.sort(key=lambda x: x.get('score', 0), reverse=True)
                
                logger.info(f"Channel {channel_name}: {len(analyzed_streams)} streams scored and sorted")
                
                # Apply provider diversification if enabled
                if self.config.get('stream_ordering', {}).get('provider_diversification', False):
                    analyzed_streams = self._apply_provider_diversification(analyzed_streams, channel_id)
                    logger.info(f"Channel {channel_name}: Applied provider diversification")
                
                # Apply account stream limits AFTER scoring (and diversification)
                if account_limits_config.get('enabled', True):
                    streams_before_limits = len(analyzed_streams)
                    analyzed_streams = self._apply_account_limits_after_scoring(
                        analyzed_streams, account_limits_config, channel_id, channel_name
                    )
                    streams_removed = streams_before_limits - len(analyzed_streams)
                    if streams_removed > 0:
                        logger.info(f"Channel {channel_name}: Removed {streams_removed} stream(s) due to account limits")
                        stats['streams_removed_by_limits'] += streams_removed
                
                streams_after = len(analyzed_streams)
                stats['total_streams_after'] += streams_after
                
                # Update channel streams if there are changes
                if streams_after != streams_before or streams_after > 0:
                    # Extract stream IDs in new order
                    new_stream_ids = [s['stream_id'] for s in analyzed_streams]
                    
                    # Update channel streams in UDI
                    from api_utils import update_channel_streams
                    success = update_channel_streams(channel_id, [{'id': sid} for sid in new_stream_ids])
                    
                    if success:
                        stats['channels_updated'] += 1
                        stats['channels_with_changes'].append({
                            'channel_id': channel_id,
                            'channel_name': channel_name,
                            'streams_before': streams_before,
                            'streams_after': streams_after,
                            'streams_removed': streams_before - streams_after
                        })
                        logger.info(f"✓ Updated channel {channel_name}: {streams_before} → {streams_after} streams")
                    else:
                        logger.error(f"Failed to update channel {channel_name}")
            
            # Calculate duration
            duration = time_module.time() - start_time
            stats['duration_seconds'] = round(duration, 2)
            
            logger.info("=" * 80)
            logger.info(f"RE-SCORE & RE-SORT COMPLETED in {duration:.2f}s")
            logger.info(f"  Channels processed: {stats['channels_processed']}")
            logger.info(f"  Channels updated: {stats['channels_updated']}")
            logger.info(f"  Total streams before: {stats['total_streams_before']}")
            logger.info(f"  Total streams after: {stats['total_streams_after']}")
            logger.info(f"  Streams removed by limits: {stats['streams_removed_by_limits']}")
            logger.info("=" * 80)
            
            stats['success'] = True
            return stats
            
        except Exception as e:
            logger.error(f"Error during re-score and re-sort: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
    
    def _apply_account_limits_after_scoring(self, analyzed_streams: List[Dict], account_limits_config: Dict, 
                                          channel_id: int, channel_name: str) -> List[Dict]:
        """Apply account stream limits AFTER quality scoring to keep only the BEST streams per account.
        
        This ensures that when limits are applied, we keep the highest-quality streams from each account
        rather than just the first ones found during matching.
        
        Args:
            analyzed_streams: List of analyzed streams sorted by score (highest first)
            account_limits_config: Account limits configuration
            channel_id: Channel ID for logging
            channel_name: Channel name for logging
            
        Returns:
            Filtered list of streams respecting account limits
        """
        if not account_limits_config.get('enabled', True):
            return analyzed_streams
        
        global_limit = account_limits_config.get('global_limit', 0)
        account_specific_limits = account_limits_config.get('account_limits', {})
        
        # If no limits are configured, return unchanged
        if global_limit == 0 and not account_specific_limits:
            return analyzed_streams
        
        # Get UDI manager to look up stream M3U accounts
        udi = get_udi_manager()
        
        # Track streams per account for this channel
        account_counts = defaultdict(int)
        limited_streams = []
        removed_count = 0
        
        logger.info(f"Applying account stream limits to {len(analyzed_streams)} analyzed streams for channel {channel_name}")
        
        # Process streams in score order (best first)
        for stream_data in analyzed_streams:
            stream_id = stream_data.get('stream_id')
            if not stream_id:
                limited_streams.append(stream_data)
                continue
            
            # Get stream to find its M3U account
            stream = udi.get_stream_by_id(stream_id)
            if not stream:
                limited_streams.append(stream_data)
                continue
            
            m3u_account = stream.get('m3u_account')
            
            # Skip custom streams (no m3u_account)
            if m3u_account is None:
                limited_streams.append(stream_data)
                continue
            
            # Determine limit for this account
            account_limit = account_specific_limits.get(str(m3u_account), global_limit)
            
            # If limit is 0, no limit applies
            if account_limit == 0:
                limited_streams.append(stream_data)
                continue
            
            # Check if we're within the limit for this account
            if account_counts[m3u_account] < account_limit:
                limited_streams.append(stream_data)
                account_counts[m3u_account] += 1
                logger.debug(f"Keeping stream {stream_id} from account {m3u_account} (score: {stream_data.get('score', 0):.2f}, {account_counts[m3u_account]}/{account_limit})")
            else:
                # Stream exceeds limit - remove it (keeping only the best ones)
                removed_count += 1
                logger.debug(f"Removing stream {stream_id} from account {m3u_account} (score: {stream_data.get('score', 0):.2f}) - exceeds limit ({account_limit})")
        
        if removed_count > 0:
            logger.info(f"Applied account stream limits: kept {len(limited_streams)} streams, removed {removed_count} lower-quality streams from channel {channel_name}")
            
            # Log per-account statistics
            for account_id, count in account_counts.items():
                account_limit = account_specific_limits.get(str(account_id), global_limit)
                if account_limit > 0:
                    logger.debug(f"  Account {account_id}: {count}/{account_limit} streams kept (best quality)")
        
        return limited_streams
    
    def _apply_provider_diversification(self, analyzed_streams: List[Dict], channel_id: int) -> List[Dict]:
        """Apply provider diversification to stream ordering for better redundancy.
        
        Instead of sorting purely by score (which groups all streams from the best provider together),
        this method interleaves streams from different providers in a round-robin fashion.
        
        Example:
            Before: [A1(0.95), A2(0.94), A3(0.93), B1(0.92), B2(0.91), C1(0.89)]
            After:  [A1(0.95), B1(0.92), C1(0.89), A2(0.94), B2(0.91), A3(0.93)]
        
        This ensures that if one provider fails, the next stream is from a different provider,
        providing better failover and redundancy.
        
        Args:
            analyzed_streams: List of analyzed streams sorted by score
            channel_id: Channel ID for logging
            
        Returns:
            Reordered list with provider diversification applied
        """
        if not analyzed_streams:
            return analyzed_streams
        
        # Get UDI manager to look up stream providers (M3U accounts)
        udi = get_udi_manager()
        
        # Group streams by provider (M3U account)
        provider_streams = defaultdict(list)
        streams_without_provider = []
        
        for stream_data in analyzed_streams:
            stream_id = stream_data.get('stream_id')
            if not stream_id:
                streams_without_provider.append(stream_data)
                continue
            
            # Get stream to find its M3U account (provider)
            stream = udi.get_stream_by_id(stream_id)
            if not stream:
                streams_without_provider.append(stream_data)
                continue
            
            m3u_account = stream.get('m3u_account')
            if m3u_account is None:
                # Custom streams without provider
                streams_without_provider.append(stream_data)
            else:
                provider_streams[m3u_account].append(stream_data)
        
        # If only one provider or no providers, return original order
        if len(provider_streams) <= 1:
            logger.debug(f"Channel {channel_id}: Only {len(provider_streams)} provider(s), skipping diversification")
            return analyzed_streams
        
        # Apply round-robin diversification
        diversified_streams = []
        max_streams_per_provider = max(len(streams) for streams in provider_streams.values())
        
        # Interleave streams from different providers
        for i in range(max_streams_per_provider):
            for provider_id in sorted(provider_streams.keys()):
                streams = provider_streams[provider_id]
                if i < len(streams):
                    diversified_streams.append(streams[i])
        
        # Append streams without provider at the end
        diversified_streams.extend(streams_without_provider)
        
        logger.info(f"Channel {channel_id}: Applied provider diversification - {len(provider_streams)} providers interleaved")
        
        return diversified_streams
    
    def apply_account_limits_to_existing_channels(self) -> Dict[str, Any]:
        """Apply current account stream limits to all existing channels without full quality check.
        
        This function goes through all channels and removes excess streams per account,
        keeping only the highest-scored streams based on existing quality data.
        Useful for applying new limits without running a full quality check.
        
        Returns:
            Dict with operation results and statistics
        """
        logger.info("Applying account stream limits to existing channels...")
        
        account_limits_config = self.config.get('account_stream_limits', {})
        if not account_limits_config.get('enabled', True):
            return {'success': False, 'error': 'Account stream limits are disabled'}
        
        global_limit = account_limits_config.get('global_limit', 0)
        account_specific_limits = account_limits_config.get('account_limits', {})
        
        if global_limit == 0 and not account_specific_limits:
            return {'success': False, 'error': 'No account limits configured'}
        
        udi = get_udi_manager()
        channels = udi.get_channels()
        
        results = {
            'success': True,
            'channels_processed': 0,
            'streams_removed': 0,
            'channels_modified': 0,
            'details': []
        }
        
        for channel in channels:
            channel_id = channel.get('id')
            channel_name = channel.get('name', f'Channel {channel_id}')
            
            if not channel_id:
                continue
            
            # Get current streams for this channel
            current_streams = udi.get_channel_streams(channel_id)
            if not current_streams or len(current_streams) <= 1:
                continue  # Skip channels with 0 or 1 streams
            
            # Convert to analyzed format with existing scores/stats
            analyzed_streams = []
            for stream in current_streams:
                stream_stats = stream.get('stream_stats', {})
                if stream_stats is None:
                    stream_stats = {}
                elif isinstance(stream_stats, str):
                    try:
                        stream_stats = json.loads(stream_stats)
                    except json.JSONDecodeError:
                        stream_stats = {}
                
                analyzed = {
                    'stream_id': stream['id'],
                    'stream_name': stream.get('name', 'Unknown'),
                    'stream_url': stream.get('url', ''),
                    'resolution': stream_stats.get('resolution', '0x0'),
                    'fps': stream_stats.get('source_fps', 0),
                    'video_codec': stream_stats.get('video_codec', 'N/A'),
                    'audio_codec': stream_stats.get('audio_codec', 'N/A'),
                    'bitrate_kbps': stream_stats.get('ffmpeg_output_bitrate', 0),
                }
                
                # Calculate score based on existing data
                score = self._calculate_stream_score(analyzed, channel_id)
                analyzed['score'] = score
                analyzed_streams.append(analyzed)
            
            # Sort by score (highest first)
            analyzed_streams.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # Apply provider diversification if enabled
            if self.config.get('stream_ordering', {}).get('provider_diversification', False):
                analyzed_streams = self._apply_provider_diversification(analyzed_streams, channel_id)
            
            # Apply account limits
            original_count = len(analyzed_streams)
            limited_streams = self._apply_account_limits_after_scoring(analyzed_streams, account_limits_config, channel_id, channel_name)
            
            if len(limited_streams) < original_count:
                # Update channel with limited streams
                limited_ids = [s['stream_id'] for s in limited_streams]
                
                try:
                    from api_utils import update_channel_streams
                    update_channel_streams(channel_id, limited_ids)
                    
                    removed_count = original_count - len(limited_streams)
                    results['streams_removed'] += removed_count
                    results['channels_modified'] += 1
                    
                    results['details'].append({
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'original_streams': original_count,
                        'remaining_streams': len(limited_streams),
                        'removed_streams': removed_count
                    })
                    
                    logger.info(f"Applied limits to channel {channel_name}: {original_count} → {len(limited_streams)} streams")
                    
                except Exception as e:
                    logger.error(f"Failed to update channel {channel_id}: {e}")
                    continue
            
            results['channels_processed'] += 1
        
        logger.info(f"Account limits application completed: {results['channels_modified']} channels modified, {results['streams_removed']} streams removed")
        return results


# Global service instance
_service_instance = None
_service_lock = threading.Lock()

def get_stream_checker_service() -> StreamCheckerService:
    """Get or create the global stream checker service instance."""
    global _service_instance
    with _service_lock:
        if _service_instance is None:
            _service_instance = StreamCheckerService()
        return _service_instance
