"""
Universal Data Index (UDI) Manager - Single source of truth for all Dispatcharr data.

The UDIManager is a singleton class that:
- Manages all data access for channels, streams, groups, logos, and M3U accounts
- Provides cached data with configurable TTL
- Supports background refresh
- Handles data persistence via JSON storage

Usage:
    from udi import get_udi_manager
    
    udi = get_udi_manager()
    
    # Initialize on startup (fetches all data)
    udi.initialize()
    
    # Get data (from cache)
    channels = udi.get_channels()
    streams = udi.get_streams()
    
    # Force refresh
    udi.refresh_all()
"""

import threading
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Set

from udi.storage import UDIStorage
from udi.fetcher import UDIFetcher
from udi.cache import UDICache

from logging_config import setup_logging

# Import at module level for better performance
from dispatcharr_config import get_dispatcharr_config

logger = setup_logging(__name__)


class UDIManager:
    """
    Universal Data Index Manager - Singleton class for all Dispatcharr data access.
    
    This class provides:
    - Centralized data access for all Dispatcharr entities
    - Automatic cache management with configurable TTL
    - Background refresh capability
    - Thread-safe operations
    """
    
    def __init__(self):
        """Initialize the UDI Manager with file-based storage."""
        # Use file-based storage only
        from udi.storage import UDIStorage
        self.storage = UDIStorage()
        logger.info("Using file storage for UDI")
        
        self.fetcher = UDIFetcher()
        self.cache = UDICache()
        
        self._initialized = False
        self._lock = threading.Lock()
        self._refresh_thread = None
        self._refresh_running = False
        
        # In-memory caches for faster access
        self._channels_cache: List[Dict[str, Any]] = []
        self._streams_cache: List[Dict[str, Any]] = []
        self._channel_groups_cache: List[Dict[str, Any]] = []
        self._logos_cache: List[Dict[str, Any]] = []
        self._m3u_accounts_cache: List[Dict[str, Any]] = []
        
        # Index caches for fast lookups
        self._channels_by_id: Dict[int, Dict[str, Any]] = {}
        self._streams_by_id: Dict[int, Dict[str, Any]] = {}
        self._streams_by_url: Dict[str, Dict[str, Any]] = {}
        self._valid_stream_ids: Set[int] = set()
        
        logger.info("UDI Manager created")
    
    def initialize(self, force_refresh: bool = False) -> bool:
        """
        Initialize the UDI Manager by loading or fetching all data.
        
        This should be called on application startup. It will:
        1. Load existing data from storage if available
        2. Fetch fresh data from the API if storage is empty or force_refresh is True
        
        Args:
            force_refresh: If True, always fetch fresh data from API
            
        Returns:
            True if initialization successful
        """
        with self._lock:
            if self._initialized and not force_refresh:
                logger.debug("UDI Manager already initialized")
                return True
            
            logger.info("Initializing UDI Manager...")
            
            # Check if we have existing data
            if not force_refresh and self.storage.is_initialized():
                logger.info("Loading existing data from storage...")
                self._load_from_storage()
                self._initialized = True
                logger.info("UDI Manager initialized from storage")
                return True
            
            # Fetch fresh data from API
            logger.info("Fetching fresh data from API...")
            try:
                success = self.refresh_all()
                if success:
                    self._initialized = True
                    logger.info("UDI Manager initialized with fresh data")
                    return True
                else:
                    logger.error("Failed to initialize UDI Manager")
                    return False
            except Exception as e:
                logger.error(f"Error initializing UDI Manager: {e}")
                return False
    
    def _load_from_storage(self) -> None:
        """Load all data from storage into memory caches."""
        self._channels_cache = self.storage.load_channels()
        self._streams_cache = self.storage.load_streams()
        self._channel_groups_cache = self.storage.load_channel_groups()
        self._logos_cache = self.storage.load_logos()
        self._m3u_accounts_cache = self.storage.load_m3u_accounts()
        
        # Build index caches
        self._build_indexes()
        
        # Mark caches as refreshed based on storage metadata
        metadata = self.storage.load_metadata()
        for entity_type in ['channels', 'streams', 'channel_groups', 'logos', 'm3u_accounts']:
            timestamp_str = metadata.get(f'{entity_type}_last_updated')
            if timestamp_str:
                try:
                    timestamp = datetime.fromisoformat(timestamp_str)
                    self.cache.mark_refreshed(entity_type, timestamp)
                except ValueError:
                    pass
        
        logger.info(
            f"Loaded from storage: {len(self._channels_cache)} channels, "
            f"{len(self._streams_cache)} streams, {len(self._channel_groups_cache)} groups, "
            f"{len(self._logos_cache)} logos, {len(self._m3u_accounts_cache)} M3U accounts"
        )
    
    def _build_indexes(self) -> None:
        """Build index caches for fast lookups."""
        self._channels_by_id = {ch.get('id'): ch for ch in self._channels_cache if ch.get('id') is not None}
        self._streams_by_id = {st.get('id'): st for st in self._streams_cache if st.get('id') is not None}
        self._streams_by_url = {st.get('url'): st for st in self._streams_cache if st.get('url')}
        self._valid_stream_ids = set(self._streams_by_id.keys())
    
    # === Data Access Methods ===
    
    def get_channels(self) -> List[Dict[str, Any]]:
        """Get all channels.
        
        Returns:
            List of channel dictionaries
        """
        self._ensure_initialized()
        return self._channels_cache.copy()
    
    def get_channel_by_id(self, channel_id: int, fetch_if_missing: bool = True) -> Optional[Dict[str, Any]]:
        """Get a specific channel by ID.
        
        If the channel is not in the cache and fetch_if_missing is True,
        attempts to fetch it from the API and add it to the cache.
        
        Args:
            channel_id: The channel ID
            fetch_if_missing: If True, fetch from API when not in cache (default: True)
            
        Returns:
            Channel dictionary or None if not found
        """
        self._ensure_initialized()
        channel = self._channels_by_id.get(channel_id)
        
        if channel is None and fetch_if_missing:
            # Channel not in cache, try fetching from API
            logger.debug(f"Channel {channel_id} not in cache, fetching from API")
            try:
                # Fetch channel from Dispatcharr API (returns channel dict or None)
                channel = self.fetcher.fetch_channel_by_id(channel_id)
                if channel:
                    # Add to caches under lock to ensure thread safety
                    with self._lock:
                        # Only add if still not in cache (could have been added by another thread)
                        if channel_id not in self._channels_by_id:
                            self._channels_by_id[channel_id] = channel
                            self._channels_cache.append(channel)
                        else:
                            # Already in cache, use the cached version
                            channel = self._channels_by_id[channel_id]
                    logger.info(f"Fetched and cached channel {channel_id}")
            except Exception as e:
                logger.warning(f"Failed to fetch channel {channel_id} from API: {e}")
                channel = None
        
        return channel
    
    def get_channel_streams(self, channel_id: int) -> List[Dict[str, Any]]:
        """Get streams for a specific channel.
        
        Args:
            channel_id: The channel ID
            
        Returns:
            List of stream dictionaries for the channel
        """
        self._ensure_initialized()
        channel = self._channels_by_id.get(channel_id)
        if not channel:
            return []
        
        stream_ids = channel.get('streams', [])
        return [self._streams_by_id.get(sid) for sid in stream_ids if sid in self._streams_by_id]
    
    def get_streams(self, log_result: bool = True) -> List[Dict[str, Any]]:
        """Get all streams.
        
        Args:
            log_result: Whether to log the number of streams returned
            
        Returns:
            List of stream dictionaries
        """
        self._ensure_initialized()
        if log_result:
            logger.info(f"Returning {len(self._streams_cache)} streams from UDI")
        return self._streams_cache.copy()
    
    def get_stream_by_id(self, stream_id: int) -> Optional[Dict[str, Any]]:
        """Get a specific stream by ID.
        
        Args:
            stream_id: The stream ID
            
        Returns:
            Stream dictionary or None if not found
        """
        self._ensure_initialized()
        return self._streams_by_id.get(stream_id)
    
    def get_stream_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        """Get a specific stream by URL.
        
        Args:
            url: The stream URL
            
        Returns:
            Stream dictionary or None if not found
        """
        self._ensure_initialized()
        return self._streams_by_url.get(url)
    
    def get_valid_stream_ids(self) -> Set[int]:
        """Get a set of all valid stream IDs.
        
        Returns:
            Set of valid stream IDs
        """
        self._ensure_initialized()
        return self._valid_stream_ids.copy()
    
    def get_channel_groups(self) -> List[Dict[str, Any]]:
        """Get all channel groups.
        
        Returns:
            List of channel group dictionaries
        """
        self._ensure_initialized()
        return self._channel_groups_cache.copy()
    
    def get_logos(self) -> List[Dict[str, Any]]:
        """Get all logos.
        
        Returns:
            List of logo dictionaries
        """
        self._ensure_initialized()
        return self._logos_cache.copy()
    
    def get_logo_by_id(self, logo_id: int) -> Optional[Dict[str, Any]]:
        """Get a specific logo by ID.
        
        Args:
            logo_id: The logo ID
            
        Returns:
            Logo dictionary or None if not found
        """
        self._ensure_initialized()
        for logo in self._logos_cache:
            if logo.get('id') == logo_id:
                return logo
        return None
    
    def get_m3u_accounts(self) -> List[Dict[str, Any]]:
        """Get all M3U accounts.
        
        Returns:
            List of M3U account dictionaries
        """
        self._ensure_initialized()
        logger.debug(f"Returning {len(self._m3u_accounts_cache)} M3U accounts from UDI cache")
        return self._m3u_accounts_cache.copy()
    
    def get_m3u_account_by_id(self, account_id: int) -> Optional[Dict[str, Any]]:
        """Get a specific M3U account by ID.
        
        Args:
            account_id: M3U account ID
            
        Returns:
            M3U account dictionary or None if not found
        """
        self._ensure_initialized()
        
        # Try fast lookup from storage if using Redis
        if hasattr(self.storage, 'get_m3u_account_by_id'):
            account = self.storage.get_m3u_account_by_id(account_id)
            if account:
                return account
        
        # Fallback to in-memory cache
        for account in self._m3u_accounts_cache:
            if account.get('id') == account_id:
                return account.copy()
        
        logger.debug(f"M3U account {account_id} not found in UDI")
        return None
    
    def has_custom_streams(self) -> bool:
        """Check if any custom streams exist.
        
        Returns:
            True if at least one custom stream exists
        """
        self._ensure_initialized()
        return any(st.get('is_custom', False) for st in self._streams_cache)
    
    # === Refresh Methods ===
    
    def refresh_all(self) -> bool:
        """Refresh all data from the API.
        
        Returns:
            True if refresh successful
        """
        logger.info("Refreshing all UDI data...")
        
        try:
            data = self.fetcher.refresh_all()
            
            # Update in-memory caches
            self._channels_cache = data.get('channels', [])
            self._streams_cache = data.get('streams', [])
            self._channel_groups_cache = data.get('channel_groups', [])
            self._logos_cache = data.get('logos', [])
            self._m3u_accounts_cache = data.get('m3u_accounts', [])
            
            # Build index caches
            self._build_indexes()
            
            # Save to storage
            self.storage.save_channels(self._channels_cache)
            self.storage.save_streams(self._streams_cache)
            self.storage.save_channel_groups(self._channel_groups_cache)
            self.storage.save_logos(self._logos_cache)
            self.storage.save_m3u_accounts(self._m3u_accounts_cache)
            
            # Update metadata
            now = datetime.now()
            metadata = {
                'last_full_refresh': now.isoformat(),
                'channels_last_updated': now.isoformat(),
                'streams_last_updated': now.isoformat(),
                'channel_groups_last_updated': now.isoformat(),
                'logos_last_updated': now.isoformat(),
                'm3u_accounts_last_updated': now.isoformat(),
                'version': '1.0.0'
            }
            self.storage.save_metadata(metadata)
            
            # Mark all caches as refreshed
            for entity_type in ['channels', 'streams', 'channel_groups', 'logos', 'm3u_accounts']:
                self.cache.mark_refreshed(entity_type, now)
            
            logger.info("UDI data refresh complete")
            return True
            
        except Exception as e:
            logger.error(f"Error refreshing UDI data: {e}")
            return False
    
    def refresh_channels(self) -> bool:
        """Refresh only channels data.
        
        Returns:
            True if refresh successful
        """
        logger.info("Refreshing channels...")
        try:
            channels = self.fetcher.fetch_channels()
            self._channels_cache = channels
            self._channels_by_id = {ch.get('id'): ch for ch in channels if ch.get('id') is not None}
            self.storage.save_channels(channels)
            self.cache.mark_refreshed('channels')
            return True
        except Exception as e:
            logger.error(f"Error refreshing channels: {e}")
            return False
    
    def refresh_channel_by_id(self, channel_id: int) -> bool:
        """Refresh a single channel by ID from the API.
        
        This is more efficient than refreshing all channels when only one channel
        needs to be updated (e.g., after modifying its stream list).
        
        Args:
            channel_id: The channel ID to refresh
            
        Returns:
            True if refresh successful
        """
        logger.info(f"Refreshing channel {channel_id}...")
        try:
            channel = self.fetcher.fetch_channel_by_id(channel_id)
            if channel:
                # Update in-memory caches
                with self._lock:
                    self._channels_by_id[channel_id] = channel
                    
                    # Update list cache
                    found = False
                    for i, ch in enumerate(self._channels_cache):
                        if ch.get('id') == channel_id:
                            self._channels_cache[i] = channel
                            found = True
                            break
                    
                    if not found:
                        self._channels_cache.append(channel)
                    
                    # Update storage
                    self.storage.update_channel(channel_id, channel)
                
                logger.info(f"Channel {channel_id} refreshed successfully")
                return True
            else:
                logger.warning(f"Failed to refresh channel {channel_id}: channel not found")
                return False
        except Exception as e:
            logger.error(f"Error refreshing channel {channel_id}: {e}")
            return False
    
    def refresh_streams(self) -> bool:
        """Refresh only streams data.
        
        Returns:
            True if refresh successful
        """
        logger.info("Refreshing streams...")
        try:
            streams = self.fetcher.fetch_streams()
            self._streams_cache = streams
            self._streams_by_id = {st.get('id'): st for st in streams if st.get('id') is not None}
            self._streams_by_url = {st.get('url'): st for st in streams if st.get('url')}
            self._valid_stream_ids = set(self._streams_by_id.keys())
            self.storage.save_streams(streams)
            self.cache.mark_refreshed('streams')
            return True
        except Exception as e:
            logger.error(f"Error refreshing streams: {e}")
            return False
    
    def refresh_channel_groups(self) -> bool:
        """Refresh only channel groups data.
        
        Returns:
            True if refresh successful
        """
        logger.info("Refreshing channel groups...")
        try:
            groups = self.fetcher.fetch_channel_groups()
            self._channel_groups_cache = groups
            self.storage.save_channel_groups(groups)
            self.cache.mark_refreshed('channel_groups')
            return True
        except Exception as e:
            logger.error(f"Error refreshing channel groups: {e}")
            return False
    
    def refresh_m3u_accounts(self) -> bool:
        """Refresh only M3U accounts data.
        
        Returns:
            True if refresh successful
        """
        logger.info("Refreshing M3U accounts...")
        try:
            accounts = self.fetcher.fetch_m3u_accounts()
            self._m3u_accounts_cache = accounts
            self.storage.save_m3u_accounts(accounts)
            self.cache.mark_refreshed('m3u_accounts')
            return True
        except Exception as e:
            logger.error(f"Error refreshing M3U accounts: {e}")
            return False
    
    def invalidate_cache(self, entity_type: Optional[str] = None) -> None:
        """Invalidate cache for entity type(s).
        
        Args:
            entity_type: Specific type to invalidate, or None for all
        """
        if entity_type:
            self.cache.invalidate(entity_type)
        else:
            self.cache.invalidate_all()
    
    # === Background Refresh ===
    
    def start_background_refresh(self, interval_seconds: int = 300) -> None:
        """Start background refresh thread.
        
        Args:
            interval_seconds: Seconds between refresh cycles
        """
        if self._refresh_running:
            logger.warning("Background refresh already running")
            return
        
        self._refresh_running = True
        
        def refresh_loop():
            logger.info(f"Starting background refresh (interval: {interval_seconds}s)")
            while self._refresh_running:
                time.sleep(interval_seconds)
                if self._refresh_running:
                    try:
                        # Refresh data that needs updating based on TTL
                        for entity_type in ['channels', 'streams', 'channel_groups', 'logos', 'm3u_accounts']:
                            if self.cache.needs_refresh(entity_type):
                                getattr(self, f'refresh_{entity_type}')()
                    except Exception as e:
                        logger.error(f"Error in background refresh: {e}")
            logger.info("Background refresh stopped")
        
        self._refresh_thread = threading.Thread(target=refresh_loop, daemon=True)
        self._refresh_thread.start()
        logger.info("Background refresh thread started")
    
    def stop_background_refresh(self) -> None:
        """Stop background refresh thread."""
        if self._refresh_running:
            self._refresh_running = False
            if self._refresh_thread:
                self._refresh_thread.join(timeout=5)
            logger.info("Background refresh stopped")
    
    # === Update Methods (for write-through) ===
    
    def update_channel(self, channel_id: int, channel_data: Dict[str, Any]) -> bool:
        """Update a channel in the cache.
        
        This is called after a successful API update to keep the cache in sync.
        
        Args:
            channel_id: The channel ID
            channel_data: The updated channel data
            
        Returns:
            True if successful
        """
        with self._lock:
            # Update in-memory cache
            self._channels_by_id[channel_id] = channel_data
            
            # Update list cache
            for i, ch in enumerate(self._channels_cache):
                if ch.get('id') == channel_id:
                    self._channels_cache[i] = channel_data
                    break
            else:
                self._channels_cache.append(channel_data)
            
            # Save to storage
            return self.storage.update_channel(channel_id, channel_data)
    
    def update_stream(self, stream_id: int, stream_data: Dict[str, Any]) -> bool:
        """Update a stream in the cache.
        
        This is called after a successful API update to keep the cache in sync.
        
        Args:
            stream_id: The stream ID
            stream_data: The updated stream data
            
        Returns:
            True if successful
        """
        with self._lock:
            # Update in-memory caches
            self._streams_by_id[stream_id] = stream_data
            if stream_data.get('url'):
                self._streams_by_url[stream_data['url']] = stream_data
            
            # Update list cache
            for i, st in enumerate(self._streams_cache):
                if st.get('id') == stream_id:
                    self._streams_cache[i] = stream_data
                    break
            else:
                self._streams_cache.append(stream_data)
                self._valid_stream_ids.add(stream_id)
            
            # Save to storage
            return self.storage.update_stream(stream_id, stream_data)
    
    # === Status Methods ===
    
    def get_status(self) -> Dict[str, Any]:
        """Get the current UDI Manager status.
        
        Returns:
            Dictionary with status information
        """
        return {
            'initialized': self._initialized,
            'background_refresh_running': self._refresh_running,
            'data_counts': {
                'channels': len(self._channels_cache),
                'streams': len(self._streams_cache),
                'channel_groups': len(self._channel_groups_cache),
                'logos': len(self._logos_cache),
                'm3u_accounts': len(self._m3u_accounts_cache)
            },
            'cache_status': self.cache.get_status(),
            'storage_path': str(self.storage.storage_dir)
        }
    
    def is_initialized(self) -> bool:
        """Check if UDI Manager is initialized.
        
        Returns:
            True if initialized
        """
        return self._initialized
    
    def _ensure_initialized(self) -> None:
        """Ensure UDI Manager is initialized before data access.
        
        This will auto-initialize if not already done, but only if
        Dispatcharr credentials are configured.
        """
        if not self._initialized:
            # Check if Dispatcharr is configured before auto-initializing
            config = get_dispatcharr_config()
            
            if not config.is_configured():
                logger.warning("UDI Manager not initialized and Dispatcharr credentials not configured. Skipping auto-initialization.")
                return
            
            logger.info("UDI Manager not initialized, auto-initializing...")
            self.initialize()


# Global singleton instance
_udi_manager: Optional[UDIManager] = None
_udi_lock = threading.Lock()


def get_udi_manager() -> UDIManager:
    """Get the global UDI Manager singleton instance.
    
    Returns:
        The UDI Manager instance
    """
    global _udi_manager
    with _udi_lock:
        if _udi_manager is None:
            _udi_manager = UDIManager()
        return _udi_manager
